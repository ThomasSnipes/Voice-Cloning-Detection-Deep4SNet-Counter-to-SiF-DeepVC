{"cells":[{"cell_type":"markdown","metadata":{"id":"L-4NEAcb5sTf"},"source":["## Choose Root Directory\n","\n","Uncomment the directory of your choice\n","- Mount Google Drive's root_dir\n","- OR define your own root_dir"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23277,"status":"ok","timestamp":1714460075167,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"AcQNqIab09iH","outputId":"71795959-b367-4e2e-ba33-2364a1ed15b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","root_dir = '/content/drive/MyDrive/'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEMdmsiP0sUt"},"outputs":[],"source":["# replace 'PATH' with your desired project directory\n","root_dir = r'D:\\Files\\Projects\\Deep Learning\\\\' # add an extra '\\' at the end"]},{"cell_type":"markdown","metadata":{"id":"8CDdpwbLWWxP"},"source":["# Download Original Model (Original-HVoice)\n","- https://github.com/yohannarodriguez/Deep4SNet\n","\n","Download the 2 following files:\n","- model_deep4SNet.h5\n","- weights_Deep4SNet.h5\n","\n","After:\n","- Run the code to create the folder that will hold the files\n","- Place both .h5 files inside the folder named 'Deep4SNet-Original-HVoice' in the model_dir"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":156,"status":"error","timestamp":1714414912942,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"3OWplzaqWe3n","outputId":"f3fef475-4a45-41fe-ea26-cc2054d7a51f"},"outputs":[{"ename":"NameError","evalue":"name 'root_dir' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-0f8a3eab34f9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Voice_Cloning_Detection/Models/Deep4SNet-Original-HVoice'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'root_dir' is not defined"]}],"source":["import os\n","\n","model_dir = root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Original-HVoice'\n","if not os.path.exists(model_dir):\n","    os.makedirs(model_dir)"]},{"cell_type":"markdown","metadata":{"id":"QjGdYj-1F6Kb"},"source":["# Data (ONLY RUN ONCE)\n","- no need to run if you have histograms stored in set folders"]},{"cell_type":"markdown","metadata":{"id":"9i7saBui1Tk8"},"source":["## Unzip datasets\n","- Modify the directories to your desire but KEEP the integrity that is after '/Voice_Cloning_Detection/'\n","- This places the project inside the /Voice_Cloning_Detection/ folder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M1slmn9tIxxD"},"outputs":[],"source":["# Create main folders\n","import os\n","sif_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC'\n","h_dir = root_dir + 'Voice_Cloning_Detection/Data/H-Voice'\n","iS_dir = root_dir + 'Voice_Cloning_Detection/Data/Import/SiF-DeepVC'\n","iH_dir = root_dir + 'Voice_Cloning_Detection/Data/Import/H-Voice'\n","\n","if not os.path.exists(sif_dir):\n","    os.makedirs(sif_dir)\n","if not os.path.exists(h_dir):\n","    os.makedirs(h_dir)\n","if not os.path.exists(iS_dir):\n","    os.makedirs(iS_dir)\n","if not os.path.exists(iH_dir):\n","    os.makedirs(iH_dir)"]},{"cell_type":"markdown","metadata":{"id":"qBqwpqQI9H-s"},"source":["### SiF-DeepVC (UNCOMMENT & RUN ONCE)\n","\n","Instructions:\n","- https://github.com/dstsmallbird/SiF-DeepVC_Dataset\n","- Download the Google Drive .zip file (7.8 GB) at the zip_dir shown below\n","\n","Structure:\n","\n","RQ1 (Part 1)\n","- \"for-real-validation\": original human recordings from FoR Validation dataset\n","- \"zh-real-test\": original human recordings from MagicData Test dataset\n","\n","RQ2 (Part 2)\n","- \"for-real-validation-denoised\": slightly denoised \"for-real-validation\"\n","- \"zh-real-test-denoised\": slightly denoised \"zh-real-test\"\n","- \"zh-real-test-silenced\": silence-removed \"zh-real-test\"\n","\n","RQ3 (Part 3)\n","- \"for-bh-madefake-final-r4k\": cloned fake voices by SiF-DeepVC for Farid et al.\n","- \"for-deep4s-madefake-final-r4k\": cloned fake voices by SiF-DeepVC for Deep4SNet\n","- \"for-rawnet-madefake-final-r4k\": cloned fake voices by SiF-DeepVC for RawNet2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1986777,"status":"ok","timestamp":1713413159149,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"v2brnV8J1SYJ","outputId":"7179397c-59e3-453e-cd5a-9ced0ae8f360"},"outputs":[],"source":["# Your .zip directory\n","zip_dir = root_dir + 'Voice_Cloning_Detection/Data/Import/SiF-DeepVC/DeepVC-Dataset.zip'\n","target_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/All'\n","\n","# # Create folder\n","import os\n","if not os.path.exists(target_dir):\n","    os.makedirs(target_dir)\n","\n","# Unzip ALL files\n","!unzip \"$zip_dir\" -d \"$target_dir\"\n"]},{"cell_type":"markdown","metadata":{"id":"GRpe7FAv9KdY"},"source":["### H-Voice (UNCOMMENT & RUN ONCE)\n","\n","Instructions:\n","- https://data.mendeley.com/datasets/k47yd3m28w/4\n","- Click the [Download All 87.8 MB] button and place the .zip file at the zip_dir shown below\n","\n","Structure:\n","- Training_fake: 2088 histograms of fake voice recordings (2016 with Imitation and with 72 Deep Voice)\n","- Training_original: 2020 histograms of original voice recordings\n","- Validation_fake: 864 histograms of fake voice recordings (all with Imitation)\n","- Validation_original: 864 histograms of original voice recordings\n","- External_test1: 760 histograms (380 original + 380 fake with Imitation)\n","- External_test2: 76 histograms (4 original + 72 fake with Deep Voice)\n","\n","Their Split:\n","- Train: 61.57%\n","- Validation: 25.90%\n","- Test: 12.53%"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"executionInfo":{"elapsed":35857,"status":"error","timestamp":1714173701812,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"Bnm-9CVK9Ts-","outputId":"af1fd262-9f91-47b4-bdc2-20c33943c78a"},"outputs":[],"source":["# Your .zip directory -------------------------------------------------------------------------------\n","zip_dir = root_dir + 'Voice_Cloning_Detection/Data/Import/H-Voice/H-Voice_Dataset.zip'\n","target_dir = root_dir + 'Voice_Cloning_Detection/Data/H-Voice/All'\n","\n","# Create folder\n","import os\n","if not os.path.exists(target_dir):\n","    print(\"made\")\n","    os.makedirs(target_dir)\n","\n","# Unzip ALL files\n","!unzip \"$zip_dir\" -d \"$target_dir\"\n","\n","# Unzip the files in the subfolders\n","import zipfile\n","for subdir, dirs, files in os.walk(target_dir):\n","    print(\"target: \", target_dir)\n","    for file in files:\n","        # Check if the file is a zip file\n","        if file.endswith('.zip'):\n","            print(\"zip\")\n","            zip_path = os.path.join(subdir, file)\n","            # Unzip the file in its current directory\n","            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","                zip_ref.extractall(subdir)\n","            # Delete the zip file\n","            #os.remove(zip_path)\n","\n","\n","# Define directories -------------------------------------------------------------------------------\n","dir_1f = root_dir + 'Voice_Cloning_Detection/Data/H-Voice/Training_Set/fake'\n","dir_1r = root_dir + 'Voice_Cloning_Detection/Data/H-Voice/Training_Set/real'\n","dir_2f = root_dir + 'Voice_Cloning_Detection/Data/H-Voice/Validation_Set/fake'\n","dir_2r = root_dir + 'Voice_Cloning_Detection/Data/H-Voice/Validation_Set/real'\n","dir_3f = root_dir + 'Voice_Cloning_Detection/Data/H-Voice/Test_Set/fake'\n","dir_3r = root_dir + 'Voice_Cloning_Detection/Data/H-Voice/Test_Set/real'\n","\n","# Create directories if they don't exist\n","for directory in [dir_1f, dir_1r, dir_2f, dir_2r, dir_3f, dir_3r]:\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","\n","# Function to move contents of a directory -------------------------------------------------------------------------------\n","import shutil\n","def move_contents(source_dir, target_dir):\n","    for item in os.listdir(source_dir):\n","        source_item = os.path.join(source_dir, item)\n","        if os.path.isfile(source_item):\n","            shutil.move(source_item, target_dir)\n","        elif os.path.isdir(source_item):\n","            # Recursively move contents of subdirectories\n","            move_contents(source_item, target_dir)\n","\n","# Move contents of source directories to target directories\n","move_contents(root_dir + 'Voice_Cloning_Detection/Data/H-Voice/All/Training_fake/Training_fake', dir_1f)\n","move_contents(root_dir + 'Voice_Cloning_Detection/Data/H-Voice/All/Training_original/Training_original', dir_1r)\n","move_contents(root_dir + 'Voice_Cloning_Detection/Data/H-Voice/All/Validation_fake/Validation_fake', dir_2f)\n","move_contents(root_dir + 'Voice_Cloning_Detection/Data/H-Voice/All/Validation_original/Validation_original', dir_2r)\n","move_contents(root_dir + 'Voice_Cloning_Detection/Data/H-Voice/All/External_test1/External_test1/FAKE', dir_3f)\n","move_contents(root_dir + 'Voice_Cloning_Detection/Data/H-Voice/All/External_test1/External_test1/ORIGINAL', dir_3r)\n","move_contents(root_dir + 'Voice_Cloning_Detection/Data/H-Voice/All/External_test2/External_test2/Fake', dir_3f)\n","move_contents(root_dir + 'Voice_Cloning_Detection/Data/H-Voice/All/External_test2/External_test2/Original', dir_3r)\n","\n","\n","# Count files after move -------------------------------------------------------------------------------\n","def count_files(directory):\n","    file_count = 0\n","    for root, dirs, files in os.walk(directory):\n","        file_count += len(files)\n","    return file_count\n","\n","counts = {\n","    'dir_1f': count_files(dir_1f),\n","    'dir_1r': count_files(dir_1r),\n","    'dir_2f': count_files(dir_2f),\n","    'dir_2r': count_files(dir_2r),\n","    'dir_3f': count_files(dir_3f),\n","    'dir_3r': count_files(dir_3r)\n","}\n","print(\"\\nFiles Count:\")\n","for directory, count in counts.items():\n","    print(f\"{directory}: {count} files\")"]},{"cell_type":"markdown","metadata":{"id":"KbpeiTpH6OV3"},"source":["## Sample data\n","* 4.5k real voices from FoR\n","* 4.5k fake voices from against Faird + RawNet\n","* 1.0k fake voices from against Deep4SNet (as testing later)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":157083,"status":"ok","timestamp":1714460241254,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"7UeiAmUP7Fw-","outputId":"774c665b-9803-4afb-fa4f-e2c04e67d621"},"outputs":[{"name":"stdout","output_type":"stream","text":["All files:\n","13410\n","8720\n","1505\n","9015\n","\n","Samples sizes:\n","4500\n","2995\n","1505\n","1000\n"]}],"source":["import os\n","import random\n","\n","# Define the directories\n","dir_real_for = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ1/for-real-validation'\n","dir_fake_farid = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ3/for-bh-madefake-final-r4k'\n","dir_fake_rawnet = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ3/for-rawnet-madefake-final-r4k'\n","dir_fake_deep4s = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ3/for-deep4s-madefake-final-r4k'\n","\n","# Get a list of all .wav file paths in the directories\n","files_real_for = [os.path.join(dir_real_for, f) for f in os.listdir(dir_real_for) if f.endswith('.wav')]\n","files_fake_farid = [os.path.join(dir_fake_farid, f) for f in os.listdir(dir_fake_farid) if f.endswith('.wav')]\n","files_fake_rawnet = [os.path.join(dir_fake_rawnet, f) for f in os.listdir(dir_fake_rawnet) if f.endswith('.wav')]\n","files_fake_deep4s = [os.path.join(dir_fake_deep4s, f) for f in os.listdir(dir_fake_deep4s) if f.endswith('.wav')]\n","\n","# check files exist\n","print(\"All files:\")\n","print(len(files_real_for))\n","print(len(files_fake_farid))\n","print(len(files_fake_rawnet))\n","print(len(files_fake_deep4s))\n","\n","# Randomly sample from each file path\n","sample_real = random.sample(files_real_for, 4500)\n","sample_fake_1 = random.sample(files_fake_farid, 2995)\n","sample_fake_2 = random.sample(files_fake_rawnet, 1505)\n","sample_fake_deep4s = random.sample(files_fake_deep4s, 1000)\n","\n","# check samples exist\n","print(\"\\nSamples sizes:\")\n","print(len(sample_real))\n","print(len(sample_fake_1))\n","print(len(sample_fake_2))\n","print(len(sample_fake_deep4s))\n"]},{"cell_type":"markdown","metadata":{"id":"BCPg229mGNRT"},"source":["## Label Samples"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":134,"status":"ok","timestamp":1714415175519,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"agwOyk-lGRSZ","outputId":"9f8f3d73-7456-455e-9e8c-f7486123c338"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of files: 9000\n","\n","Labels for the first 3 files:\n","/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ1/for-real-validation/38_5870_20170917090222.wav -> real\n","/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ1/for-real-validation/38_5856_20170916204033.wav -> real\n","/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ1/for-real-validation/38_5889_20170917215434.wav -> real\n","\n","Labels for the last 3 files:\n","/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ3/for-rawnet-madefake-final-r4k/file22076.wav_16k.wav_norm.wav_mono.wav_silence.wav_00.wav.noisered.wav -> fake\n","/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ3/for-rawnet-madefake-final-r4k/file21672.wav_16k.wav_norm.wav_mono.wav_silence.wav_04.wav.noisered.wav -> fake\n","/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ3/for-rawnet-madefake-final-r4k/file33129.wav_16k.wav_norm.wav_mono.wav_silence.wav_03.wav.noisered.wav -> fake\n","\n","Total number of files in data_deep4s: 1000\n","\n","Labels for the first 3 files in data_deep4s:\n","/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ3/for-deep4s-madefake-final-r4k/file32757.wav_16k.wav_norm.wav_mono.wav_silence.wav_00.wav.noisered.wav -> fake\n","/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ3/for-deep4s-madefake-final-r4k/file26960.wav_16k.wav_norm.wav_mono.wav_silence.wav_02.wav.noisered.wav -> fake\n","/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ3/for-deep4s-madefake-final-r4k/file24477.wav_16k.wav_norm.wav_mono.wav_silence.wav_02.wav.noisered.wav -> fake\n","\n","Labels for the last 3 files in data_deep4s:\n","/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ3/for-deep4s-madefake-final-r4k/file23763.wav_16k.wav_norm.wav_mono.wav_silence.wav_01.wav.noisered.wav -> fake\n","/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ3/for-deep4s-madefake-final-r4k/file17995.wav_16k.wav_norm.wav_mono.wav_silence.wav_02.wav.noisered.wav -> fake\n","/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ3/for-deep4s-madefake-final-r4k/file4991.wav_16k.wav_norm.wav_mono.wav_silence.wav_04.wav.noisered.wav -> fake\n"]}],"source":["# Create a dictionary to store file paths and labels\n","data = {}\n","data_deep4s = {}\n","\n","# Assign labels\n","for file_path in sample_real:\n","    data[file_path] = 'real'\n","for file_path in sample_fake_1 + sample_fake_2:\n","    data[file_path] = 'fake'\n","for file_path in sample_fake_deep4s:\n","    data_deep4s[file_path] = 'fake'\n","\n","# Check some files and their labels\n","print(\"Total number of files:\", len(data))\n","\n","print(\"\\nLabels for the first 3 files:\")\n","for file_path, label in list(data.items())[:3]:\n","    print(file_path, \"->\", label)\n","\n","print(\"\\nLabels for the last 3 files:\")\n","for file_path, label in list(data.items())[8997:]:\n","    print(file_path, \"->\", label)\n","\n","# Check some files and their labels in data_deep4s\n","print(\"\\nTotal number of files in data_deep4s:\", len(data_deep4s))\n","\n","print(\"\\nLabels for the first 3 files in data_deep4s:\")\n","for file_path, label in list(data_deep4s.items())[:3]:\n","    print(file_path, \"->\", label)\n","\n","print(\"\\nLabels for the last 3 files in data_deep4s:\")\n","for file_path, label in list(data_deep4s.items())[-3:]:\n","    print(file_path, \"->\", label)"]},{"cell_type":"markdown","metadata":{"id":"pW8BFSlRD0v3"},"source":["## Split data\n","* Train: 70%\n","* Validation: 15%\n","* Test: 15%"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135,"status":"ok","timestamp":1714415178166,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"xPuSRhG7D8Y9","outputId":"ecbc8c30-cfe5-4e19-b79d-4ac841d1ec2c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set size: 6300\n","Validation set size: 1350\n","Testing set size: 1350\n","Deep4S set size: 1000\n"]}],"source":["import random\n","\n","# Shuffle the dictionary keys\n","keys = list(data.keys())\n","random.shuffle(keys)\n","deep4s_keys = list(data_deep4s.keys())\n","random.shuffle(deep4s_keys)\n","\n","# Calculate the sizes of each set\n","total_size = len(keys)\n","train_size = int(total_size * 0.7)\n","val_size = int(total_size * 0.15)\n","\n","# Divide the keys into training, validation, and testing sets\n","train_keys = keys[:train_size]\n","val_keys = keys[train_size:train_size + val_size]\n","test_keys = keys[train_size + val_size:]\n","\n","# Retrieve the corresponding file paths and labels for each set\n","train_set = [(key, data[key]) for key in train_keys]\n","val_set = [(key, data[key]) for key in val_keys]\n","test_set = [(key, data[key]) for key in test_keys]\n","deep4s_set = [(key, data_deep4s[key]) for key in deep4s_keys]\n","\n","# Print sizes of each set\n","print(\"Training set size:\", len(train_set))\n","print(\"Validation set size:\", len(val_set))\n","print(\"Testing set size:\", len(test_set))\n","print(\"Deep4S set size:\", len(deep4s_set))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1713994969819,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"nXCAm7nsHvdm","outputId":"96ce5d3e-602f-414d-db5f-3ee97e287b7f"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," ('/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ1/for-real-validation/38_5883_20170917153723.wav', 'real')\n","\n"," ('/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ1/for-real-validation/38_5867_20170917125935.wav', 'real')\n","\n"," ('/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ1/for-real-validation/38_5890_20170918124921.wav', 'real')\n","\n"," ('/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ3/for-deep4s-madefake-final-r4k/file12645.wav_16k.wav_norm.wav_mono.wav_silence.wav_03.wav.noisered.wav', 'fake')\n"]}],"source":["# Check 1 example from each\n","print(\"\\n\", train_set[0])\n","print(\"\\n\", val_set[0])\n","print(\"\\n\", test_set[0])\n","print(\"\\n\", deep4s_set[0])"]},{"cell_type":"markdown","metadata":{"id":"GuMznXbEGHZ9"},"source":["# Feature Extraction (ONLY RUN ONCE)\n","- no need to run if you have histograms stored in set folders\n","- extracts histograms from SiF-DeepVC, not H-Voice. H-Voice already comes with histograms."]},{"cell_type":"markdown","metadata":{"id":"cShfFKg_qheO"},"source":["## Histograms"]},{"cell_type":"markdown","metadata":{"id":"jgSK2LUssJ7q"},"source":["### Regular Function\n","- no limitations\n","- imiate H-voice histograms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zyFNlUjqZAOe"},"outputs":[],"source":["import numpy as np\n","import librosa.display\n","import matplotlib.pyplot as plt\n","def compute_histogram(file_path, dir, iter):\n","    # Load audio file\n","    audio, sr = librosa.load(file_path, sr=None)\n","\n","    # Calculate histogram of audio\n","    hist, bins = np.histogram(audio, bins=256, range=(-1, 1)) # 2^8 bins\n","\n","    # Plot histogram\n","    plt.figure()\n","    plt.bar(bins[:-1], hist, width=(bins[1] - bins[0]), color='black')\n","    #plt.title('Histogram of Audio')\n","    #plt.xlabel('Amplitude')\n","    #plt.ylabel('Frequency')\n","    plt.savefig(os.path.join(dir, f'hist_{iter}.png'))\n","    plt.close()\n","    #plt.show()\n","    #print(hist.shape)\n","    #print(hist.dtype)\n","    return hist"]},{"cell_type":"markdown","metadata":{"id":"xJpYr9ECXUJI"},"source":["### Filtered Function - Limit Histograms under 4 kHz (WIP)\n","3. Create a more generalized model by training our model on both H-Voice and SiF-DeepVC data sets.\n","4. Limit the histograms to below 4000 Hz. Since the SiF-DeepVC's handcrafted SiFs were designed at above 4k Hz, we want to test the model's capabilities when ignoring the SiFs."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1714460028459,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"Loy9Rd8RQ13C"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import librosa.display\n","\n","# Function to plot spectrogram\n","def plot_spectrogram(audio, sr, title):\n","    plt.figure(figsize=(10, 4))\n","    spectrogram = librosa.display.specshow(librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max), sr=sr, x_axis='time', y_axis='log')\n","    plt.colorbar(format='%+2.0f dB')\n","    plt.title(title)\n","    plt.tight_layout()\n","    plt.show()\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2672,"status":"ok","timestamp":1714460034625,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"SZiEjBYCJBd9"},"outputs":[],"source":["import numpy as np\n","import scipy.signal\n","import scipy.io.wavfile\n","\n","def filter(audio_data, cutoff_frequency, sr):\n","    # Define the filter\n","    nyquist_frequency = sr / 2\n","    cutoff_normalized = cutoff_frequency / nyquist_frequency\n","    b, a = scipy.signal.butter(4, cutoff_normalized, btype='low')\n","\n","    # Apply the filter to each channel\n","    filtered_audio = np.apply_along_axis(lambda x: scipy.signal.filtfilt(b, a, x), axis=0, arr=audio_data)\n","\n","    return filtered_audio\n","\n","# Load the original audio file\n","#sampling_rate, audio_data = scipy.io.wavfile.read(file_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180},"executionInfo":{"elapsed":803,"status":"error","timestamp":1714460042887,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"3uXqc9a4q0k8","outputId":"6f47d96e-28ea-4ba6-bd56-56c83811a348"},"outputs":[],"source":["import numpy as np\n","import librosa\n","import librosa.display\n","import matplotlib.pyplot as plt\n","import os\n","def compute_histogram_filtered(file_path, dir, iter):\n","    # Load audio file\n","    audio, sr = librosa.load(file_path, sr=44100)\n","    # Plot spectrogram of original audio\n","    plot_spectrogram(audio, 44100, title='Original Audio Spectrogram')\n","\n","\n","    cutoff_frequency = 4000\n","    filtered_audio = filter(audio, cutoff_frequency, sr=44100)\n","    # Plot spectrogram of filtered audio\n","    plot_spectrogram(filtered_audio, 44100, title='Filtered Audio Spectrogram')\n","    # Calculate histogram of audio\n","    hist, bins = np.histogram(filtered_audio, bins=256, range=(-1, 1)) # 2^8 bins\n","\n","    # Plot histogram\n","    plt.figure()\n","    plt.bar(bins[:-1], hist, width=(bins[1] - bins[0]), color='black')\n","    #plt.title('Histogram of Audio')\n","    #plt.xlabel('Amplitude')\n","    #plt.ylabel('Frequency')\n","    #plt.savefig(os.path.join(dir, f'hist_{iter}.png'))\n","    plt.show()\n","    plt.close()\n","    #print(hist.shape)\n","    #print(hist.dtype)\n","    return hist\n","\n","# Test\n","plot_spectrogram(test_set[0][0], \"/content/drive/MyDrive/Voice_Cloning_Detection/Data/Test\", 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":554},"executionInfo":{"elapsed":12352,"status":"ok","timestamp":1714014772743,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"xw4lV6FNgJi4","outputId":"8d961203-8af4-4eba-dd8f-df31c0e6984f"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/All/DeepVC-Dataset/RQ3/for-deep4s-madefake-final-r4k/file11711.wav_16k.wav_norm.wav_mono.wav_silence.wav_02.wav.noisered.wav\n"]},{"name":"stderr","output_type":"stream","text":["Exception ignored in: <function _xla_gc_callback at 0x79897dab88b0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 98, in _xla_gc_callback\n","    def _xla_gc_callback(*args):\n","KeyboardInterrupt: \n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# TEST CASE\n","\n","def compute_histogram_filtered_test(file_path):\n","    # Load audio file\n","    audio, sr = librosa.load(file_path, sr=None)\n","\n","    # Calculate histogram of audio\n","    hist, bins = np.histogram(audio, bins=256, range=(-1, 1)) # Ours: 2^8 | Original: 2^16 bins\n","\n","    # Plot histogram\n","    plt.figure()\n","    plt.bar(bins[:-1], hist, width=(bins[1] - bins[0]), color='black')\n","    #plt.title('{}'.format(file_path))\n","    #plt.xlabel('Amplitude')\n","    #plt.ylabel('Frequency')\n","    plt.show()\n","    plt.close()\n","    #print(hist.shape)\n","    #print(hist.dtype)\n","\n","# Test them\n","fileInput = deep4s_set[0][0]\n","print(fileInput)\n","compute_histogram_filtered_test(fileInput)\n"]},{"cell_type":"markdown","metadata":{"id":"gfpp0gybcWBo"},"source":["### Store Histograms - Regular"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":769},"executionInfo":{"elapsed":295376,"status":"error","timestamp":1713998387917,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"iFUxjhJPciji","outputId":"c92c0534-cabd-482c-9387-2cf0b4930389"},"outputs":[],"source":["# Training Set\n","#   This must run in batches. Trying to create 4200 graphs will crash the program due to a lack of RAM.\n","#   Batches can be as large as 900.\n","import os\n","\n","save_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Training_Set/'\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","\n","batch_size = 900\n","num_batches = len(train_set) // batch_size  # Calculate the number of batches\n","start_index = 3900\n","end_index = 4199\n","\n","for i in range(batch_size):\n","    #start_index = batch_index * batch_size\n","    #end_index = (batch_index + 1) * batch_size\n","\n","    for i, (file_path, label) in enumerate(train_set[start_index:end_index]):\n","        label_dir = os.path.join(save_dir, label)\n","        if not os.path.isdir(label_dir):\n","            os.makedirs(label_dir)  # Create the directory if it doesn't exist\n","\n","        hist = compute_histogram(file_path, label_dir, i + start_index)\n","\n","# Process the remaining items (if any) after the last full batch\n","#remaining_items = len(train_set) % batch_size\n","#if remaining_items > 0:\n","    #start_index = num_batches * batch_size\n","    #for i, (file_path, label) in enumerate(train_set[start_index:]):\n","        #label_dir = os.path.join(save_dir, label)\n","        #if not os.path.isdir(label_dir):\n","            #os.makedirs(label_dir)  # Create the directory if it doesn't exist\n","\n","        #hist = compute_histogram(file_path, label_dir, i + start_index)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LL0SaqeccjsP"},"outputs":[],"source":["# Validation Set\n","save_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Validation_Set/'\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","\n","iter = 0;\n","\n","for file_path, label in val_set:\n","    label_dir = os.path.join(save_dir, label)\n","    if not os.path.isdir(label_dir):\n","      os.makedirs(label_dir)  # Create the directory if it doesn't exist\n","\n","    hist = compute_histogram(file_path, label_dir, iter)\n","    iter+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1kllzFlclrG"},"outputs":[],"source":["# Test Set\n","save_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Test_Set/'\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","\n","iter = 0;\n","\n","for file_path, label in test_set:\n","    label_dir = os.path.join(save_dir, label)\n","    if not os.path.isdir(label_dir):\n","      os.makedirs(label_dir)  # Create the directory if it doesn't exist\n","\n","    hist = compute_histogram(file_path, label_dir, iter)\n","    iter+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zifep8lUcne6"},"outputs":[],"source":["# Deep4S Test Set - SiF-DeepVC built to bypass Deep4SNet\n","save_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Deep4SNet_Target_Test_Set/'\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","\n","iter = 0;\n","\n","for file_path, label in deep4s_set:\n","    label_dir = os.path.join(save_dir, label)\n","    if not os.path.isdir(label_dir):\n","      os.makedirs(label_dir)  # Create the directory if it doesn't exist\n","\n","    hist = compute_histogram(file_path, label_dir, iter)\n","    iter+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":474,"status":"ok","timestamp":1714004501616,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"5Lmjav0wcp7s","outputId":"5c0b6109-7f00-4f94-df6a-092c11affaef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory: /content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/Training_Set/\n","   Subdirectory: real | Histograms: 2094\n","   Subdirectory: fake | Histograms: 2099\n","Total histograms:  4193\n","\n","Directory: /content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/Validation_Set/\n","   Subdirectory: real | Histograms: 680\n","   Subdirectory: fake | Histograms: 670\n","Total histograms:  1350\n","\n","Directory: /content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/Test_Set/\n","   Subdirectory: real | Histograms: 673\n","   Subdirectory: fake | Histograms: 677\n","Total histograms:  1350\n","\n","Directory: /content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/Deep4SNet_Target_Test_Set/\n","   Subdirectory: fake | Histograms: 1000\n","Total histograms:  1000\n","\n"]}],"source":["# Count histograms in each directory\n","def count_png_files(directory):\n","    count = 0\n","    for root, dirs, files in os.walk(directory):\n","        for file in files:\n","            if file.endswith(\".png\"):\n","                count += 1\n","    return count\n","\n","directories = [\n","    root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Training_Set/',\n","    root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Validation_Set/',\n","    root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Test_Set/',\n","    root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Deep4SNet_Target_Test_Set/'\n","]\n","\n","for directory in directories:\n","    total_count = 0\n","    print(\"Directory:\", directory)\n","    for sub_dir in os.listdir(directory):\n","        sub_dir_path = os.path.join(directory, sub_dir)\n","        if os.path.isdir(sub_dir_path):\n","            png_count = count_png_files(sub_dir_path)\n","            total_count += png_count\n","            print(\"   Subdirectory:\", sub_dir, \"| Histograms:\", png_count)\n","    print(\"Total histograms: \", total_count)\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"YaLtJPW6rydg"},"source":["### Store Histograms - Filtered"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":774},"executionInfo":{"elapsed":1694785,"status":"error","timestamp":1714016576635,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"10VQVDV4rxxN","outputId":"e3c49a05-8bf6-49f6-ee24-568015cef056"},"outputs":[],"source":["# Training Set\n","#   This must run in batches. Trying to create 4200 graphs will crash the program due to a lack of RAM.\n","#   Batches can be as large as 900.\n","\n","save_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Training_Set_Filtered/'\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","\n","batch_size = 1940\n","num_batches = len(train_set) // batch_size  # Calculate the number of batches\n","start_index = 2260\n","end_index = 4199\n","\n","for i in range(batch_size):\n","    #start_index = batch_index * batch_size\n","    #end_index = (batch_index + 1) * batch_size\n","\n","    for i, (file_path, label) in enumerate(train_set[start_index:end_index]):\n","        label_dir = os.path.join(save_dir, label)\n","        if not os.path.isdir(label_dir):\n","            os.makedirs(label_dir)  # Create the directory if it doesn't exist\n","\n","        hist = compute_histogram(file_path, label_dir, i + start_index)\n","\n","# Process the remaining items (if any) after the last full batch\n","#remaining_items = len(train_set) % batch_size\n","#if remaining_items > 0:\n","    #start_index = num_batches * batch_size\n","    #for i, (file_path, label) in enumerate(train_set[start_index:]):\n","        #label_dir = os.path.join(save_dir, label)\n","        #if not os.path.isdir(label_dir):\n","            #os.makedirs(label_dir)  # Create the directory if it doesn't exist\n","\n","        #hist = compute_histogram(file_path, label_dir, i + start_index)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WOWdasmQCnjz"},"outputs":[],"source":["# Validation Set\n","save_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Validation_Set_Filtered/'\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","\n","iter = 0;\n","\n","for file_path, label in val_set:\n","    label_dir = os.path.join(save_dir, label)\n","    if not os.path.isdir(label_dir):\n","      os.makedirs(label_dir)  # Create the directory if it doesn't exist\n","\n","    hist = compute_histogram(file_path, label_dir, iter)\n","    iter+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICPdktBiCnvR"},"outputs":[],"source":["# Test Set\n","save_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Test_Set_Filtered/'\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","\n","iter = 0;\n","\n","for file_path, label in test_set:\n","    label_dir = os.path.join(save_dir, label)\n","    if not os.path.isdir(label_dir):\n","      os.makedirs(label_dir)  # Create the directory if it doesn't exist\n","\n","    hist = compute_histogram(file_path, label_dir, iter)\n","    iter+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eAi_uqNOurte"},"outputs":[],"source":["# Deep4S Test Set - SiF-DeepVC built to bypass Deep4SNet\n","save_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Deep4SNet_Target_Test_Set_Filtered/'\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","\n","iter = 0;\n","\n","for file_path, label in deep4s_set:\n","    label_dir = os.path.join(save_dir, label)\n","    if not os.path.isdir(label_dir):\n","      os.makedirs(label_dir)  # Create the directory if it doesn't exist\n","\n","    hist = compute_histogram(file_path, label_dir, iter)\n","    iter+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":869,"status":"ok","timestamp":1714021632065,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"-mnTav8dhNyy","outputId":"334f3bb4-e23c-4f86-d765-5f1694a3caca"},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory: /content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/Training_Set_Filtered/\n","   Subdirectory: real | Histograms: 2082\n","   Subdirectory: fake | Histograms: 2116\n","Total histograms:  4198\n","\n","Directory: /content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/Validation_Set_Filtered/\n","   Subdirectory: fake | Histograms: 701\n","   Subdirectory: real | Histograms: 649\n","Total histograms:  1350\n","\n","Directory: /content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/Test_Set_Filtered/\n","   Subdirectory: fake | Histograms: 686\n","   Subdirectory: real | Histograms: 664\n","Total histograms:  1350\n","\n","Directory: /content/drive/MyDrive/Voice_Cloning_Detection/Data/SiF-DeepVC/Deep4SNet_Target_Test_Set/\n","   Subdirectory: fake | Histograms: 1000\n","Total histograms:  1000\n","\n"]}],"source":["# Count histograms in each directory\n","def count_png_files(directory):\n","    count = 0\n","    for root, dirs, files in os.walk(directory):\n","        for file in files:\n","            if file.endswith(\".png\"):\n","                count += 1\n","    return count\n","\n","directories = [\n","    root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Training_Set_Filtered/',\n","    root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Validation_Set_Filtered/',\n","    root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Test_Set_Filtered/',\n","    root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Deep4SNet_Target_Test_Set/'\n","]\n","\n","for directory in directories:\n","    total_count = 0\n","    print(\"Directory:\", directory)\n","    for sub_dir in os.listdir(directory):\n","        sub_dir_path = os.path.join(directory, sub_dir)\n","        if os.path.isdir(sub_dir_path):\n","            png_count = count_png_files(sub_dir_path)\n","            total_count += png_count\n","            print(\"   Subdirectory:\", sub_dir, \"| Histograms:\", png_count)\n","    print(\"Total histograms: \", total_count)\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"fXJpfi1R7AQC"},"source":["# Format Features"]},{"cell_type":"markdown","metadata":{"id":"BZ2YQ9HPrm1J"},"source":["### Format Histograms - SiF-DeepVC - Regular\n","- need to run to generate inputs for training, validation, and testing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30801,"status":"ok","timestamp":1714415242282,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"z1a2hhtKrrpm","outputId":"582dc68b-a4b7-4e8f-d497-1bb152d7f04f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 4193 images belonging to 2 classes.\n","Found 1350 images belonging to 2 classes.\n","Found 1350 images belonging to 2 classes.\n","Found 1000 images belonging to 1 classes.\n"]}],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","\n","# ---------- Format training set ---------- #\n","train_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Training_Set'\n","\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,        # Normalize pixel values to [0, 1]\n","    horizontal_flip=True,  # Enable horizontal flip augmentation for training data\n",")\n","\n","# Flow training images in batches of 32 using train_datagen generator\n","train_generator_SiF_reg = train_datagen.flow_from_directory(\n","    train_dir,              # Path to the training directory\n","    target_size=(150, 150), # Resize images to 150x150 pixels\n","    batch_size=32,\n","    class_mode='binary'     # Assuming binary classification\n",")\n","\n","\n","# ---------- Format validation set ---------- #\n","validation_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Validation_Set'\n","\n","valid_datagen = ImageDataGenerator(\n","    rescale=1./255,  # Normalize pixel values to [0, 1] for validation data\n",")\n","\n","# Flow validation images in batches of 32 using valid_datagen generator\n","valid_generator_SiF_reg = valid_datagen.flow_from_directory(\n","    validation_dir,         # Path to the validation directory\n","    target_size=(150, 150), # Resize images to 150x150 pixels\n","    batch_size=32,\n","    class_mode='binary'     # Assuming binary classification\n",")\n","\n","\n","# ---------- Format test set ---------- #\n","test_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Test_Set'\n","\n","test_datagen = ImageDataGenerator(\n","    rescale=1./255,  # Normalize pixel values to [0, 1] for validation data\n",")\n","\n","# Flow validation images in batches of 32 using valid_datagen generator\n","test_generator_SiF_reg = test_datagen.flow_from_directory(\n","    test_dir,         # Path to the validation directory\n","    target_size=(150, 150), # Resize images to 150x150 pixels\n","    batch_size=32,\n","    class_mode='binary'     # Assuming binary classification\n",")\n","\n","\n","# ---------- Format deep4s test set ---------- #\n","deep4s_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Deep4SNet_Target_Test_Set'\n","\n","deep4s_datagen = ImageDataGenerator(\n","    rescale=1./255,  # Normalize pixel values to [0, 1] for validation data\n",")\n","\n","# Flow validation images in batches of 32 using valid_datagen generator\n","deep4s_generator_SiF_reg = deep4s_datagen.flow_from_directory(\n","    deep4s_dir,         # Path to the validation directory\n","    target_size=(150, 150), # Resize images to 150x150 pixels\n","    batch_size=32,\n","    class_mode='binary'     # Assuming binary classification\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125,"status":"ok","timestamp":1714415328160,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"OT0TotfuHKvl","outputId":"e73c7045-ac91-4300-d7ef-df2c752a58ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0]\n"]}],"source":["classes = valid_generator_SiF_reg.classes\n","\n","# Assuming 0 corresponds to real audio and 1 corresponds to fake audio\n","y_val = classes\n","print(y_val[100:250])"]},{"cell_type":"markdown","metadata":{"id":"tppoDtKQqKEb"},"source":["## Format Histograms - SiF-DeepVC - Filtered (WIP)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25342,"status":"ok","timestamp":1714327518506,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"NSXOiVhBKC0V","outputId":"66a08162-2456-40d5-bf16-336330937ac8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 4198 images belonging to 2 classes.\n","Found 1350 images belonging to 2 classes.\n","Found 1350 images belonging to 2 classes.\n"]}],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","\n","# ---------- Format training set ---------- #\n","train_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Training_Set_Filtered'\n","\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,        # Normalize pixel values to [0, 1]\n","    horizontal_flip=True,  # Enable horizontal flip augmentation for training data\n",")\n","\n","# Flow training images in batches of 32 using train_datagen generator\n","train_generator_SiF_filt = train_datagen.flow_from_directory(\n","    train_dir,              # Path to the training directory\n","    target_size=(150, 150), # Resize images to 150x150 pixels\n","    batch_size=32,\n","    class_mode='binary'     # Assuming binary classification\n",")\n","\n","\n","# ---------- Format validation set ---------- #\n","validation_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Validation_Set_Filtered'\n","\n","valid_datagen = ImageDataGenerator(\n","    rescale=1./255,  # Normalize pixel values to [0, 1] for validation data\n",")\n","\n","# Flow validation images in batches of 32 using valid_datagen generator\n","valid_generator_SiF_filt = valid_datagen.flow_from_directory(\n","    validation_dir,         # Path to the validation directory\n","    target_size=(150, 150), # Resize images to 150x150 pixels\n","    batch_size=32,\n","    class_mode='binary'     # Assuming binary classification\n",")\n","\n","\n","# ---------- Format test set ---------- #\n","test_dir = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Test_Set_Filtered'\n","\n","test_datagen = ImageDataGenerator(\n","    rescale=1./255,  # Normalize pixel values to [0, 1] for validation data\n",")\n","\n","# Flow validation images in batches of 32 using valid_datagen generator\n","test_generator_SiF_filt = test_datagen.flow_from_directory(\n","    test_dir,         # Path to the validation directory\n","    target_size=(150, 150), # Resize images to 150x150 pixels\n","    batch_size=32,\n","    class_mode='binary'     # Assuming binary classification\n",")"]},{"cell_type":"markdown","metadata":{"id":"SP8zoYANqAgX"},"source":["## Format Histograms - H-Voice"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23653,"status":"ok","timestamp":1714327545022,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"wPwPjE9DqNOF","outputId":"05b25488-76a5-4748-a344-d89cea9c35f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 4108 images belonging to 2 classes.\n","Found 1728 images belonging to 2 classes.\n","Found 836 images belonging to 2 classes.\n"]}],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","\n","# ---------- Format training set ---------- #\n","train_dir = root_dir + 'Voice_Cloning_Detection/Data/H-Voice/Training_Set'\n","\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,        # Normalize pixel values to [0, 1]\n","    horizontal_flip=True,  # Enable horizontal flip augmentation for training data\n",")\n","\n","# Flow training images in batches of 32 using train_datagen generator\n","train_generator_HVoice = train_datagen.flow_from_directory(\n","    train_dir,              # Path to the training directory\n","    target_size=(150, 150), # Resize images to 150x150 pixels\n","    batch_size=32,\n","    class_mode='binary'     # Assuming binary classification\n",")\n","\n","\n","# ---------- Format validation set ---------- #\n","validation_dir = root_dir + 'Voice_Cloning_Detection/Data/H-Voice/Validation_Set'\n","\n","valid_datagen = ImageDataGenerator(\n","    rescale=1./255,  # Normalize pixel values to [0, 1] for validation data\n",")\n","\n","# Flow validation images in batches of 32 using valid_datagen generator\n","valid_generator_HVoice = valid_datagen.flow_from_directory(\n","    validation_dir,         # Path to the validation directory\n","    target_size=(150, 150), # Resize images to 150x150 pixels\n","    batch_size=32,\n","    class_mode='binary'     # Assuming binary classification\n",")\n","\n","\n","# ---------- Format test set ---------- #\n","test_dir = root_dir + 'Voice_Cloning_Detection/Data/H-Voice/Test_Set'\n","\n","test_datagen = ImageDataGenerator(\n","    rescale=1./255,  # Normalize pixel values to [0, 1] for validation data\n",")\n","\n","# Flow validation images in batches of 32 using valid_datagen generator\n","test_generator_HVoice = test_datagen.flow_from_directory(\n","    test_dir,         # Path to the validation directory\n","    target_size=(150, 150), # Resize images to 150x150 pixels\n","    batch_size=32,\n","    class_mode='binary'     # Assuming binary classification\n",")"]},{"cell_type":"markdown","metadata":{"id":"1Ebnj90fWhHR"},"source":["## Format Histograms - H-Voice + SiF-DeepVC (Regular)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41424,"status":"ok","timestamp":1714421517470,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"PvXJhOkOWud9","outputId":"2a85b2b2-2c4f-4491-97ae-922222668561"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 8300 images belonging to 2 classes.\n","Found 3078 images belonging to 2 classes.\n","Found 2186 images belonging to 2 classes.\n"]}],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","\n","# ---------- Format training set ---------- #\n","train_dir = root_dir + 'Voice_Cloning_Detection/Data/H-Voice_SiF-Regular/Training_Set'\n","\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,        # Normalize pixel values to [0, 1]\n","    horizontal_flip=True,  # Enable horizontal flip augmentation for training data\n",")\n","\n","# Flow training images in batches of 32 using train_datagen generator\n","train_generator_H_Reg = train_datagen.flow_from_directory(\n","    train_dir,              # Path to the training directory\n","    target_size=(150, 150), # Resize images to 150x150 pixels\n","    batch_size=32,\n","    class_mode='binary'     # Assuming binary classification\n",")\n","\n","\n","# ---------- Format validation set ---------- #\n","validation_dir = root_dir + 'Voice_Cloning_Detection/Data/H-Voice_SiF-Regular/Validation_Set'\n","\n","valid_datagen = ImageDataGenerator(\n","    rescale=1./255,  # Normalize pixel values to [0, 1] for validation data\n",")\n","\n","# Flow validation images in batches of 32 using valid_datagen generator\n","valid_generator_H_Reg = valid_datagen.flow_from_directory(\n","    validation_dir,         # Path to the validation directory\n","    target_size=(150, 150), # Resize images to 150x150 pixels\n","    batch_size=32,\n","    class_mode='binary'     # Assuming binary classification\n",")\n","\n","\n","# ---------- Format test set ---------- #\n","test_dir = root_dir + 'Voice_Cloning_Detection/Data/H-Voice_SiF-Regular/Test_Set'\n","\n","test_datagen = ImageDataGenerator(\n","    rescale=1./255,  # Normalize pixel values to [0, 1] for validation data\n",")\n","\n","# Flow validation images in batches of 32 using valid_datagen generator\n","test_generator_H_Reg = test_datagen.flow_from_directory(\n","    test_dir,         # Path to the validation directory\n","    target_size=(150, 150), # Resize images to 150x150 pixels\n","    batch_size=32,\n","    class_mode='binary'     # Assuming binary classification\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":216,"status":"error","timestamp":1714414907139,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"QQNBLwdoFy8e","outputId":"40dd5d7c-2924-445e-bd14-ab624cc28ea0"},"outputs":[],"source":["# Extract the classes (labels) from the generator\n","classes = valid_generator_H_Reg.classes\n","\n","# Assuming 0 corresponds to real audio and 1 corresponds to fake audio\n","y_val = classes\n","print(y_val)"]},{"cell_type":"markdown","metadata":{"id":"SMcI-Bt1wMJQ"},"source":["## Format Histograms - H-Voice + SiF-DeepVC (Filtered)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42402,"status":"ok","timestamp":1714327646780,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"924avB0gwTYm","outputId":"303b1cf7-77ca-454a-8b5e-38945efa398e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 8306 images belonging to 2 classes.\n","Found 3078 images belonging to 2 classes.\n","Found 2186 images belonging to 2 classes.\n"]}],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","\n","# ---------- Format training set ---------- #\n","train_dir = root_dir + 'Voice_Cloning_Detection/Data/H-Voice_SiF-Filtered/Training_Set'\n","\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,        # Normalize pixel values to [0, 1]\n","    horizontal_flip=True,  # Enable horizontal flip augmentation for training data\n",")\n","\n","# Flow training images in batches of 32 using train_datagen generator\n","train_generator_H_Filt = train_datagen.flow_from_directory(\n","    train_dir,              # Path to the training directory\n","    target_size=(150, 150), # Resize images to 150x150 pixels\n","    batch_size=32,\n","    class_mode='binary'     # Assuming binary classification\n",")\n","\n","\n","# ---------- Format validation set ---------- #\n","validation_dir = root_dir + 'Voice_Cloning_Detection/Data/H-Voice_SiF-Filtered/Validation_Set'\n","\n","valid_datagen = ImageDataGenerator(\n","    rescale=1./255,  # Normalize pixel values to [0, 1] for validation data\n",")\n","\n","# Flow validation images in batches of 32 using valid_datagen generator\n","valid_generator_H_Filt = valid_datagen.flow_from_directory(\n","    validation_dir,         # Path to the validation directory\n","    target_size=(150, 150), # Resize images to 150x150 pixels\n","    batch_size=32,\n","    class_mode='binary'     # Assuming binary classification\n",")\n","\n","\n","# ---------- Format test set ---------- #\n","test_dir = root_dir + 'Voice_Cloning_Detection/Data/H-Voice_SiF-Filtered/Test_Set'\n","\n","test_datagen = ImageDataGenerator(\n","    rescale=1./255,  # Normalize pixel values to [0, 1] for validation data\n",")\n","\n","# Flow validation images in batches of 32 using valid_datagen generator\n","test_generator_H_Filt = test_datagen.flow_from_directory(\n","    test_dir,         # Path to the validation directory\n","    target_size=(150, 150), # Resize images to 150x150 pixels\n","    batch_size=32,\n","    class_mode='binary'     # Assuming binary classification\n",")"]},{"cell_type":"markdown","metadata":{"id":"xWIUgZGl9Et_"},"source":[]},{"cell_type":"markdown","metadata":{"id":"LugK-d2v-S5k"},"source":["# Modeling\n","- Deep4SNet CNN\n","- https://www.sciencedirect.com/science/article/pii/S0957417421008770"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOWsX5y7-o_b"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.optimizers import RMSprop\n","from sklearn.metrics import confusion_matrix\n","\n","# Define the Deep4SNet CNN model\n","def create_cnn_model(input_shape, num_classes):\n","    model = Sequential([\n","        Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=input_shape, kernel_initializer='random_normal', bias_initializer='zeros'),\n","        MaxPooling2D((2, 2), strides=(2, 2), padding='valid'),\n","\n","        Conv2D(32, (3, 3), strides=(1, 1), activation='relu'),\n","        MaxPooling2D((2, 2), strides=(2, 2), padding='valid'),\n","\n","        Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n","        MaxPooling2D((2, 2), strides=(2, 2), padding='valid'),\n","\n","        Flatten(),\n","        Dense(64, activation='relu'),\n","        Dropout(0.2),\n","        Dense(num_classes, activation='sigmoid')  # Output layer\n","    ])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2907132,"status":"error","timestamp":1714418454444,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"r1KvYQ9bHrIt","outputId":"3d9dafaf-559b-4682-8bcf-9d56ab68fea5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","132/132 [==============================] - 1286s 10s/step - loss: 0.3667 - accuracy: 0.8691\n","Epoch 2/10\n","132/132 [==============================] - 163s 1s/step - loss: 0.1146 - accuracy: 0.9609\n","Epoch 3/10\n","132/132 [==============================] - 157s 1s/step - loss: 0.0924 - accuracy: 0.9714\n","Epoch 4/10\n","132/132 [==============================] - 155s 1s/step - loss: 0.0789 - accuracy: 0.9750\n","Epoch 5/10\n","132/132 [==============================] - 155s 1s/step - loss: 0.0704 - accuracy: 0.9759\n","Epoch 6/10\n","132/132 [==============================] - 157s 1s/step - loss: 0.0613 - accuracy: 0.9797\n","Epoch 7/10\n","132/132 [==============================] - 157s 1s/step - loss: 0.0554 - accuracy: 0.9814\n","Epoch 8/10\n","132/132 [==============================] - 155s 1s/step - loss: 0.0520 - accuracy: 0.9828\n","Epoch 9/10\n","132/132 [==============================] - 155s 1s/step - loss: 0.0426 - accuracy: 0.9855\n","Epoch 10/10\n","132/132 [==============================] - 154s 1s/step - loss: 0.0439 - accuracy: 0.9843\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 150, 150, 32)      896       \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 75, 75, 32)        0         \n"," D)                                                              \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 73, 73, 32)        9248      \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  (None, 36, 36, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 34, 34, 64)        18496     \n","                                                                 \n"," max_pooling2d_2 (MaxPoolin  (None, 17, 17, 64)        0         \n"," g2D)                                                            \n","                                                                 \n"," flatten (Flatten)           (None, 18496)             0         \n","                                                                 \n"," dense (Dense)               (None, 64)                1183808   \n","                                                                 \n"," dropout (Dropout)           (None, 64)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 1212513 (4.63 MB)\n","Trainable params: 1212513 (4.63 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]},{"ename":"ValueError","evalue":"`y` argument is not supported when using `keras.utils.Sequence` as input.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-af00ff3b2253>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Evaluate model on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mevaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_our_SiF_regular\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_generator_SiF_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Calculate predictions on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     ):\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_none_or_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1012\u001b[0m                 \u001b[0;34m\"`y` argument is not supported when using \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m                 \u001b[0;34m\"`keras.utils.Sequence` as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: `y` argument is not supported when using `keras.utils.Sequence` as input."]}],"source":["from sklearn.metrics import confusion_matrix\n","\n","# Model params\n","input_shape = (150, 150, 3)\n","num_classes = 1 # Number of outputs per input\n","\n","# Model params\n","input_shape = (150, 150, 3)\n","num_classes = 1 # Number of outputs per input\n","\n","# Model trained on training set\n","model_our_SiF_regular = create_cnn_model(input_shape, num_classes)\n","model_our_SiF_regular.compile(optimizer=RMSprop(learning_rate=0.001),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model_our_SiF_regular.fit(train_generator_SiF_reg, epochs=10)\n","model_our_SiF_regular.summary()  # Print model summary\n","model_our_SiF_regular.save(root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Our-SiF-Regular')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":495423,"status":"ok","timestamp":1714419030893,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"wqZeWr0mTfFB","outputId":"369d3464-a3c3-438a-9dff-e101843bc14f"},"outputs":[{"name":"stdout","output_type":"stream","text":["43/43 [==============================] - 418s 10s/step - loss: 0.0494 - accuracy: 0.9859\n","43/43 [==============================] - 26s 592ms/step\n","Evaluation Loss: 0.049368008971214294\n","Evaluation Accuracy: 0.9859259128570557\n","False Positive Rate (FPR): 0.5208955223880597\n"]}],"source":["# Evaluate model on validation set\n","evaluation = model_our_SiF_regular.evaluate(valid_generator_SiF_reg)\n","\n","# Calculate predictions on validation set\n","y_pred = model_our_SiF_regular.predict(valid_generator_SiF_reg)\n","\n","# Convert probabilities to binary predictions\n","y_pred_binary = (y_pred > 0.5).astype(int)\n","\n","# Calculate confusion matrix\n","conf_matrix = confusion_matrix(y_val, y_pred_binary)\n","\n","# Extract TN, FP, FN, TP from confusion matrix\n","TN = conf_matrix[0, 0]\n","FP = conf_matrix[0, 1]\n","FN = conf_matrix[1, 0]\n","TP = conf_matrix[1, 1]\n","\n","# Calculate false positive rate (FPR)\n","FPR = FP / (FP + TN)\n","\n","print(\"Evaluation Loss:\", evaluation[0])\n","print(\"Evaluation Accuracy:\", evaluation[1])\n","print(\"False Positive Rate (FPR):\", FPR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tguq2s4l2YKG"},"outputs":[],"source":["# Define the Deep4SNet CNN model\n","def create_new_cnn_model(input_shape, num_classes):\n","    model = Sequential([\n","        Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=input_shape, kernel_initializer='random_normal', bias_initializer='zeros'),\n","        MaxPooling2D((2, 2), strides=(2, 2), padding='valid'),\n","        Dropout(0.25),\n","\n","        Conv2D(32, (3, 3), strides=(1, 1), activation='relu'),\n","        MaxPooling2D((2, 2), strides=(2, 2), padding='valid'),\n","        Dropout(0.25),\n","\n","        Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n","        MaxPooling2D((2, 2), strides=(2, 2), padding='valid'),\n","        Dropout(0.25),\n","\n","        Flatten(),\n","        Dense(64, activation='relu'),\n","        Dropout(0.5),\n","        Dense(num_classes, activation='sigmoid')  # Output layer\n","    ])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zB9uLmggvZ65"},"outputs":[],"source":["# Adjusted model with increased depth and dropout rates\n","def create_adjusted_cnn_model(input_shape, num_classes):\n","    model = Sequential([\n","        Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=input_shape, kernel_initializer='random_normal', bias_initializer='zeros'),\n","        MaxPooling2D((2, 2), strides=(2, 2), padding='valid'),\n","        Dropout(0.3),  # Increased dropout rate\n","\n","        Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n","        MaxPooling2D((2, 2), strides=(2, 2), padding='valid'),\n","        Dropout(0.4),  # Increased dropout rate\n","\n","        Conv2D(128, (3, 3), strides=(1, 1), activation='relu'),\n","        MaxPooling2D((2, 2), strides=(2, 2), padding='valid'),\n","        Dropout(0.4),  # Increased dropout rate\n","\n","        Flatten(),\n","        Dense(128, activation='relu'),\n","        Dropout(0.5),  # Keep dropout rate for the dense layer\n","        Dense(num_classes, activation='sigmoid')\n","    ])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AY8_TJhXvf78"},"outputs":[],"source":["# Adjusted model with increased depth\n","def create_deeper_cnn_model(input_shape, num_classes):\n","    model = Sequential([\n","        Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=input_shape, kernel_initializer='random_normal', bias_initializer='zeros'),\n","        Conv2D(32, (3, 3), strides=(1, 1), activation='relu'),\n","        MaxPooling2D((2, 2), strides=(2, 2), padding='valid'),\n","        Dropout(0.25),\n","\n","        Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu'),\n","        Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n","        MaxPooling2D((2, 2), strides=(2, 2), padding='valid'),\n","        Dropout(0.25),\n","\n","        Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'),\n","        Conv2D(128, (3, 3), strides=(1, 1), activation='relu'),\n","        MaxPooling2D((2, 2), strides=(2, 2), padding='valid'),\n","        Dropout(0.25),\n","\n","        Flatten(),\n","        Dense(256, activation='relu'),\n","        Dropout(0.5),\n","        Dense(num_classes, activation='sigmoid')\n","    ])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cacIzIgBbWvR"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n","from tensorflow.keras.optimizers import RMSprop\n","\n","# Define the Deep4SNet CNN model\n","def batch_cnn_model(input_shape, num_classes):\n","    model = Sequential([\n","        Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=input_shape, kernel_initializer='random_normal', bias_initializer='zeros'),\n","        BatchNormalization(),\n","        MaxPooling2D((2, 2), strides=(2, 2), padding='valid'),\n","\n","\n","        Conv2D(32, (3, 3), strides=(1, 1), activation='relu'),\n","        BatchNormalization(),\n","        MaxPooling2D((2, 2), strides=(2, 2), padding='valid'),\n","\n","        Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n","        BatchNormalization(),\n","        MaxPooling2D((2, 2), strides=(2, 2), padding='valid'),\n","\n","        Flatten(),\n","        Dense(64, activation='relu'),\n","        Dropout(0.2),\n","        Dense(num_classes, activation='sigmoid')  # Output layer\n","    ])\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"hQSG2b3LJhis"},"source":["## Original Model - Loaded"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5o26vMAJhD9"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","model_path = root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Original-HVoice/model_Deep4SNet.h5'\n","weights_path = root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Original-HVoice/weights_Deep4SNet.h5'\n","model_original_HVoice = load_model(model_path)\n","model_original_HVoice.load_weights(weights_path)"]},{"cell_type":"markdown","metadata":{"id":"gtJn2rwv39kh"},"source":["## Our Model - SiF-DeepVC - Filtered\n","- Trained on filtered SiF-DeepVC histograms"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2109485,"status":"ok","timestamp":1714083604392,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"6eTY-b5J4ANl","outputId":"68563cde-f64e-4a85-83f2-2bea366d8042"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","132/132 [==============================] - 684s 5s/step - loss: 0.2461 - accuracy: 0.8878\n","Epoch 2/10\n","132/132 [==============================] - 148s 1s/step - loss: 0.1065 - accuracy: 0.9647\n","Epoch 3/10\n","132/132 [==============================] - 145s 1s/step - loss: 0.0961 - accuracy: 0.9667\n","Epoch 4/10\n","132/132 [==============================] - 145s 1s/step - loss: 0.0760 - accuracy: 0.9728\n","Epoch 5/10\n","132/132 [==============================] - 141s 1s/step - loss: 0.0663 - accuracy: 0.9793\n","Epoch 6/10\n","132/132 [==============================] - 143s 1s/step - loss: 0.0547 - accuracy: 0.9795\n","Epoch 7/10\n","132/132 [==============================] - 141s 1s/step - loss: 0.0547 - accuracy: 0.9807\n","Epoch 8/10\n","132/132 [==============================] - 143s 1s/step - loss: 0.0508 - accuracy: 0.9838\n","Epoch 9/10\n","132/132 [==============================] - 144s 1s/step - loss: 0.0477 - accuracy: 0.9845\n","Epoch 10/10\n","132/132 [==============================] - 142s 1s/step - loss: 0.0511 - accuracy: 0.9838\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 150, 150, 32)      896       \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 75, 75, 32)        0         \n"," D)                                                              \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 73, 73, 32)        9248      \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  (None, 36, 36, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 34, 34, 64)        18496     \n","                                                                 \n"," max_pooling2d_2 (MaxPoolin  (None, 17, 17, 64)        0         \n"," g2D)                                                            \n","                                                                 \n"," flatten (Flatten)           (None, 18496)             0         \n","                                                                 \n"," dense (Dense)               (None, 64)                1183808   \n","                                                                 \n"," dropout (Dropout)           (None, 64)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 1212513 (4.63 MB)\n","Trainable params: 1212513 (4.63 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# Model params\n","input_shape = (150, 150, 3)\n","num_classes = 1 # Number of outputs per input\n","\n","# Model trained on training set\n","model_our_SiF_filt = create_cnn_model(input_shape, num_classes)\n","model_our_SiF_filt.compile(optimizer=RMSprop(learning_rate=0.001),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model_our_SiF_filt.fit(train_generator_SiF_filt, epochs=10)\n","model_our_SiF_filt.summary()  # Print model summary\n","model_our_SiF_filt.save(root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Our-SiF-Filtered')"]},{"cell_type":"markdown","metadata":{"id":"RHh3JfxXIv3b"},"source":["## Our Model - SiF-DeepVC - Regular\n","- Trained on unfiltered SiF-DeepVC histograms"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1879229,"status":"ok","timestamp":1714421348228,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"GJc4rjs-H5uu","outputId":"49325338-a8e7-43ff-b300-2dd6d2a7729f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","132/132 [==============================] - 168s 1s/step - loss: 0.4421 - accuracy: 0.7982\n","Epoch 2/10\n","132/132 [==============================] - 168s 1s/step - loss: 0.1423 - accuracy: 0.9513\n","Epoch 3/10\n","132/132 [==============================] - 163s 1s/step - loss: 0.1285 - accuracy: 0.9597\n","Epoch 4/10\n","132/132 [==============================] - 165s 1s/step - loss: 0.1141 - accuracy: 0.9637\n","Epoch 5/10\n","132/132 [==============================] - 165s 1s/step - loss: 0.1069 - accuracy: 0.9673\n","Epoch 6/10\n","132/132 [==============================] - 163s 1s/step - loss: 0.0968 - accuracy: 0.9692\n","Epoch 7/10\n","132/132 [==============================] - 162s 1s/step - loss: 0.0964 - accuracy: 0.9695\n","Epoch 8/10\n","132/132 [==============================] - 164s 1s/step - loss: 0.0854 - accuracy: 0.9728\n","Epoch 9/10\n","132/132 [==============================] - 165s 1s/step - loss: 0.0824 - accuracy: 0.9754\n","Epoch 10/10\n","132/132 [==============================] - 165s 1s/step - loss: 0.0843 - accuracy: 0.9769\n","Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_6 (Conv2D)           (None, 150, 150, 32)      896       \n","                                                                 \n"," max_pooling2d_6 (MaxPoolin  (None, 75, 75, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_2 (Dropout)         (None, 75, 75, 32)        0         \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 73, 73, 32)        9248      \n","                                                                 \n"," max_pooling2d_7 (MaxPoolin  (None, 36, 36, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_3 (Dropout)         (None, 36, 36, 32)        0         \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 34, 34, 64)        18496     \n","                                                                 \n"," max_pooling2d_8 (MaxPoolin  (None, 17, 17, 64)        0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_4 (Dropout)         (None, 17, 17, 64)        0         \n","                                                                 \n"," flatten_2 (Flatten)         (None, 18496)             0         \n","                                                                 \n"," dense_4 (Dense)             (None, 64)                1183808   \n","                                                                 \n"," dropout_5 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 1212513 (4.63 MB)\n","Trainable params: 1212513 (4.63 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# Model params\n","input_shape = (150, 150, 3)\n","num_classes = 1 # Number of outputs per input\n","\n","# Model trained on training set\n","model_our_new_SiF_regular = create_new_cnn_model(input_shape, num_classes)\n","model_our_new_SiF_regular.compile(optimizer=RMSprop(learning_rate=0.001),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model_our_new_SiF_regular.fit(train_generator_SiF_reg, epochs=10)\n","model_our_new_SiF_regular.summary()  # Print model summary\n","model_our_new_SiF_regular.save(root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Our-SiF-Regular')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50775,"status":"ok","timestamp":1714421406106,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"ZmUAz3gFczdp","outputId":"13e72934-d713-4d46-87db-578a79c1a0a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["43/43 [==============================] - 24s 550ms/step - loss: 0.0558 - accuracy: 0.9763\n","43/43 [==============================] - 26s 588ms/step\n","Evaluation Loss: 0.055775467306375504\n","Evaluation Accuracy: 0.9762963056564331\n","False Positive Rate (FPR): 0.5283582089552239\n"]}],"source":["# Evaluate model on validation set\n","evaluation = model_our_new_SiF_regular.evaluate(valid_generator_SiF_reg)\n","\n","# Calculate predictions on validation set\n","y_pred = model_our_new_SiF_regular.predict(valid_generator_SiF_reg)\n","\n","# Convert probabilities to binary predictions\n","y_pred_binary = (y_pred > 0.5).astype(int)\n","\n","# Calculate confusion matrix\n","conf_matrix = confusion_matrix(y_val, y_pred_binary)\n","\n","# Extract TN, FP, FN, TP from confusion matrix\n","TN = conf_matrix[0, 0]\n","FP = conf_matrix[0, 1]\n","FN = conf_matrix[1, 0]\n","TP = conf_matrix[1, 1]\n","\n","# Calculate false positive rate (FPR)\n","FPR = FP / (FP + TN)\n","\n","print(\"Evaluation Loss:\", evaluation[0])\n","print(\"Evaluation Accuracy:\", evaluation[1])\n","print(\"False Positive Rate (FPR):\", FPR)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5791327,"status":"ok","timestamp":1714347760362,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"WGTDK_RjItT_","outputId":"35f33371-1c25-4a67-f6dc-07551374335f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","132/132 [==============================] - 569s 4s/step - loss: 0.5775 - accuracy: 0.6296\n","Epoch 2/10\n","132/132 [==============================] - 569s 4s/step - loss: 0.1282 - accuracy: 0.9611\n","Epoch 3/10\n","132/132 [==============================] - 574s 4s/step - loss: 0.1118 - accuracy: 0.9633\n","Epoch 4/10\n","132/132 [==============================] - 572s 4s/step - loss: 0.0934 - accuracy: 0.9721\n","Epoch 5/10\n","132/132 [==============================] - 570s 4s/step - loss: 0.0819 - accuracy: 0.9740\n","Epoch 6/10\n","132/132 [==============================] - 572s 4s/step - loss: 0.0772 - accuracy: 0.9759\n","Epoch 7/10\n","132/132 [==============================] - 576s 4s/step - loss: 0.0693 - accuracy: 0.9783\n","Epoch 8/10\n","132/132 [==============================] - 592s 4s/step - loss: 0.0686 - accuracy: 0.9778\n","Epoch 9/10\n","132/132 [==============================] - 572s 4s/step - loss: 0.0686 - accuracy: 0.9771\n","Epoch 10/10\n","132/132 [==============================] - 574s 4s/step - loss: 0.0589 - accuracy: 0.9809\n","Model: \"sequential_10\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_30 (Conv2D)          (None, 150, 150, 32)      896       \n","                                                                 \n"," conv2d_31 (Conv2D)          (None, 148, 148, 32)      9248      \n","                                                                 \n"," max_pooling2d_30 (MaxPooli  (None, 74, 74, 32)        0         \n"," ng2D)                                                           \n","                                                                 \n"," dropout_34 (Dropout)        (None, 74, 74, 32)        0         \n","                                                                 \n"," conv2d_32 (Conv2D)          (None, 74, 74, 64)        18496     \n","                                                                 \n"," conv2d_33 (Conv2D)          (None, 72, 72, 64)        36928     \n","                                                                 \n"," max_pooling2d_31 (MaxPooli  (None, 36, 36, 64)        0         \n"," ng2D)                                                           \n","                                                                 \n"," dropout_35 (Dropout)        (None, 36, 36, 64)        0         \n","                                                                 \n"," conv2d_34 (Conv2D)          (None, 36, 36, 128)       73856     \n","                                                                 \n"," conv2d_35 (Conv2D)          (None, 34, 34, 128)       147584    \n","                                                                 \n"," max_pooling2d_32 (MaxPooli  (None, 17, 17, 128)       0         \n"," ng2D)                                                           \n","                                                                 \n"," dropout_36 (Dropout)        (None, 17, 17, 128)       0         \n","                                                                 \n"," flatten_10 (Flatten)        (None, 36992)             0         \n","                                                                 \n"," dense_20 (Dense)            (None, 256)               9470208   \n","                                                                 \n"," dropout_37 (Dropout)        (None, 256)               0         \n","                                                                 \n"," dense_21 (Dense)            (None, 1)                 257       \n","                                                                 \n","=================================================================\n","Total params: 9757473 (37.22 MB)\n","Trainable params: 9757473 (37.22 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# Model params\n","input_shape = (150, 150, 3)\n","num_classes = 1 # Number of outputs per input\n","\n","# Model trained on training set\n","model_our_SiF_regular = create_deeper_cnn_model(input_shape, num_classes)\n","model_our_SiF_regular.compile(optimizer=RMSprop(learning_rate=0.001),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model_our_SiF_regular.fit(train_generator_SiF_reg, epochs=10)\n","model_our_SiF_regular.summary()  # Print model summary\n","model_our_SiF_regular.save(root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Our-SiF-Regular')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1676552,"status":"ok","timestamp":1714333122113,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"Bsco4t7l3feD","outputId":"e1ce5e77-d565-4ec6-ccfd-14d2e61f6d40"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","132/132 [==============================] - 154s 1s/step - loss: 0.4145 - accuracy: 0.8278\n","Epoch 2/10\n","132/132 [==============================] - 153s 1s/step - loss: 0.1200 - accuracy: 0.9599\n","Epoch 3/10\n","132/132 [==============================] - 155s 1s/step - loss: 0.1109 - accuracy: 0.9652\n","Epoch 4/10\n","132/132 [==============================] - 154s 1s/step - loss: 0.0963 - accuracy: 0.9678\n","Epoch 5/10\n","132/132 [==============================] - 154s 1s/step - loss: 0.0864 - accuracy: 0.9723\n","Epoch 6/10\n","132/132 [==============================] - 153s 1s/step - loss: 0.0764 - accuracy: 0.9742\n","Epoch 7/10\n","132/132 [==============================] - 154s 1s/step - loss: 0.0733 - accuracy: 0.9785\n","Epoch 8/10\n","132/132 [==============================] - 152s 1s/step - loss: 0.0691 - accuracy: 0.9790\n","Epoch 9/10\n","132/132 [==============================] - 151s 1s/step - loss: 0.0660 - accuracy: 0.9809\n","Epoch 10/10\n","132/132 [==============================] - 150s 1s/step - loss: 0.0631 - accuracy: 0.9804\n","Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_18 (Conv2D)          (None, 150, 150, 32)      896       \n","                                                                 \n"," max_pooling2d_18 (MaxPooli  (None, 75, 75, 32)        0         \n"," ng2D)                                                           \n","                                                                 \n"," dropout_24 (Dropout)        (None, 75, 75, 32)        0         \n","                                                                 \n"," conv2d_19 (Conv2D)          (None, 73, 73, 32)        9248      \n","                                                                 \n"," max_pooling2d_19 (MaxPooli  (None, 36, 36, 32)        0         \n"," ng2D)                                                           \n","                                                                 \n"," dropout_25 (Dropout)        (None, 36, 36, 32)        0         \n","                                                                 \n"," conv2d_20 (Conv2D)          (None, 34, 34, 64)        18496     \n","                                                                 \n"," max_pooling2d_20 (MaxPooli  (None, 17, 17, 64)        0         \n"," ng2D)                                                           \n","                                                                 \n"," dropout_26 (Dropout)        (None, 17, 17, 64)        0         \n","                                                                 \n"," flatten_6 (Flatten)         (None, 18496)             0         \n","                                                                 \n"," dense_12 (Dense)            (None, 64)                1183808   \n","                                                                 \n"," dropout_27 (Dropout)        (None, 64)                0         \n","                                                                 \n"," dense_13 (Dense)            (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 1212513 (4.63 MB)\n","Trainable params: 1212513 (4.63 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# ---------- NEW CNN MODEL ---------- #\n","\n","# Model params\n","input_shape = (150, 150, 3)\n","num_classes = 1 # Number of outputs per input\n","\n","# Model trained on training set\n","model_our_SiF_regular = create_new_cnn_model(input_shape, num_classes)\n","model_our_SiF_regular.compile(optimizer=RMSprop(learning_rate=0.001),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model_our_SiF_regular.fit(train_generator_SiF_reg, epochs=10)\n","model_our_SiF_regular.summary()  # Print model summary\n","model_our_SiF_regular.save(root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Our-SiF-Regular')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3446111,"status":"ok","timestamp":1714351375434,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"Q-CN6lY2GPU-","outputId":"93365f24-c965-4376-d0bb-360c21b1501f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","132/132 [==============================] - 337s 3s/step - loss: 0.5465 - accuracy: 0.8016\n","Epoch 2/10\n","132/132 [==============================] - 349s 3s/step - loss: 0.1685 - accuracy: 0.9490\n","Epoch 3/10\n","132/132 [==============================] - 324s 2s/step - loss: 0.1235 - accuracy: 0.9604\n","Epoch 4/10\n","132/132 [==============================] - 332s 3s/step - loss: 0.1148 - accuracy: 0.9628\n","Epoch 5/10\n","132/132 [==============================] - 324s 2s/step - loss: 0.1097 - accuracy: 0.9626\n","Epoch 6/10\n","132/132 [==============================] - 332s 3s/step - loss: 0.0985 - accuracy: 0.9678\n","Epoch 7/10\n","132/132 [==============================] - 327s 2s/step - loss: 0.0878 - accuracy: 0.9716\n","Epoch 8/10\n","132/132 [==============================] - 335s 3s/step - loss: 0.0874 - accuracy: 0.9731\n","Epoch 9/10\n","132/132 [==============================] - 324s 2s/step - loss: 0.0755 - accuracy: 0.9754\n","Epoch 10/10\n","132/132 [==============================] - 330s 2s/step - loss: 0.0760 - accuracy: 0.9728\n","Model: \"sequential_11\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_36 (Conv2D)          (None, 150, 150, 64)      1792      \n","                                                                 \n"," max_pooling2d_33 (MaxPooli  (None, 75, 75, 64)        0         \n"," ng2D)                                                           \n","                                                                 \n"," dropout_38 (Dropout)        (None, 75, 75, 64)        0         \n","                                                                 \n"," conv2d_37 (Conv2D)          (None, 73, 73, 64)        36928     \n","                                                                 \n"," max_pooling2d_34 (MaxPooli  (None, 36, 36, 64)        0         \n"," ng2D)                                                           \n","                                                                 \n"," dropout_39 (Dropout)        (None, 36, 36, 64)        0         \n","                                                                 \n"," conv2d_38 (Conv2D)          (None, 34, 34, 128)       73856     \n","                                                                 \n"," max_pooling2d_35 (MaxPooli  (None, 17, 17, 128)       0         \n"," ng2D)                                                           \n","                                                                 \n"," dropout_40 (Dropout)        (None, 17, 17, 128)       0         \n","                                                                 \n"," flatten_11 (Flatten)        (None, 36992)             0         \n","                                                                 \n"," dense_22 (Dense)            (None, 128)               4735104   \n","                                                                 \n"," dropout_41 (Dropout)        (None, 128)               0         \n","                                                                 \n"," dense_23 (Dense)            (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 4847809 (18.49 MB)\n","Trainable params: 4847809 (18.49 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# ---------- NEW CNN MODEL ---------- #\n","\n","# Model params\n","input_shape = (150, 150, 3)\n","num_classes = 1 # Number of outputs per input\n","\n","# Model trained on training set\n","model_our_SiF_regular = create_adjusted_cnn_model(input_shape, num_classes)\n","model_our_SiF_regular.compile(optimizer=RMSprop(learning_rate=0.001),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model_our_SiF_regular.fit(train_generator_SiF_reg, epochs=10)\n","model_our_SiF_regular.summary()  # Print model summary\n","model_our_SiF_regular.save(root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Our-SiF-Regular')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1882863,"status":"ok","timestamp":1714364408873,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"hAU4GVqxb7iB","outputId":"cb1885f1-5615-4143-bd74-8c41a7c6d9f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","132/132 [==============================] - 189s 1s/step - loss: 0.7503 - accuracy: 0.9161\n","Epoch 2/10\n","132/132 [==============================] - 187s 1s/step - loss: 0.1576 - accuracy: 0.9535\n","Epoch 3/10\n","132/132 [==============================] - 186s 1s/step - loss: 0.1135 - accuracy: 0.9692\n","Epoch 4/10\n","132/132 [==============================] - 187s 1s/step - loss: 0.0954 - accuracy: 0.9731\n","Epoch 5/10\n","132/132 [==============================] - 185s 1s/step - loss: 0.0952 - accuracy: 0.9731\n","Epoch 6/10\n","132/132 [==============================] - 188s 1s/step - loss: 0.0752 - accuracy: 0.9781\n","Epoch 7/10\n","132/132 [==============================] - 185s 1s/step - loss: 0.0745 - accuracy: 0.9797\n","Epoch 8/10\n","132/132 [==============================] - 185s 1s/step - loss: 0.0636 - accuracy: 0.9804\n","Epoch 9/10\n","132/132 [==============================] - 185s 1s/step - loss: 0.0602 - accuracy: 0.9812\n","Epoch 10/10\n","132/132 [==============================] - 186s 1s/step - loss: 0.0586 - accuracy: 0.9816\n","Model: \"sequential_14\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_45 (Conv2D)          (None, 150, 150, 32)      896       \n","                                                                 \n"," batch_normalization_2 (Bat  (None, 150, 150, 32)      128       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_42 (MaxPooli  (None, 75, 75, 32)        0         \n"," ng2D)                                                           \n","                                                                 \n"," conv2d_46 (Conv2D)          (None, 73, 73, 32)        9248      \n","                                                                 \n"," batch_normalization_3 (Bat  (None, 73, 73, 32)        128       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_43 (MaxPooli  (None, 36, 36, 32)        0         \n"," ng2D)                                                           \n","                                                                 \n"," conv2d_47 (Conv2D)          (None, 34, 34, 64)        18496     \n","                                                                 \n"," batch_normalization_4 (Bat  (None, 34, 34, 64)        256       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_44 (MaxPooli  (None, 17, 17, 64)        0         \n"," ng2D)                                                           \n","                                                                 \n"," flatten_14 (Flatten)        (None, 18496)             0         \n","                                                                 \n"," dense_28 (Dense)            (None, 64)                1183808   \n","                                                                 \n"," dropout_44 (Dropout)        (None, 64)                0         \n","                                                                 \n"," dense_29 (Dense)            (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 1213025 (4.63 MB)\n","Trainable params: 1212769 (4.63 MB)\n","Non-trainable params: 256 (1.00 KB)\n","_________________________________________________________________\n"]}],"source":["# Model params\n","input_shape = (150, 150, 3)\n","num_classes = 1 # Number of outputs per input\n","\n","# Model trained on training set\n","model_our_SiF_regular = batch_cnn_model(input_shape, num_classes)\n","model_our_SiF_regular.compile(optimizer=RMSprop(learning_rate=0.001),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model_our_SiF_regular.fit(train_generator_SiF_reg, epochs=10)\n","model_our_SiF_regular.summary()  # Print model summary\n","model_our_SiF_regular.save(root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Our-SiF-Regular')"]},{"cell_type":"markdown","metadata":{"id":"1bmQ901jpe1r"},"source":["## Our Model - H-Voice\n","- Closest imitation to the loaded original model\n","- Trained on H-Voice histograms"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1152849,"status":"ok","timestamp":1713243338094,"user":{"displayName":"Shuang Lin","userId":"13512369623226330440"},"user_tz":240},"id":"JJEIuQuMpq9i","outputId":"64300858-2070-4ab8-c4da-4585e59a39c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","129/129 [==============================] - 912s 7s/step - loss: 0.7177 - accuracy: 0.5027\n","Epoch 2/10\n","129/129 [==============================] - 23s 181ms/step - loss: 0.6932 - accuracy: 0.5088\n","Epoch 3/10\n","129/129 [==============================] - 24s 190ms/step - loss: 0.6931 - accuracy: 0.5083\n","Epoch 4/10\n","129/129 [==============================] - 24s 189ms/step - loss: 0.6931 - accuracy: 0.5083\n","Epoch 5/10\n","129/129 [==============================] - 23s 181ms/step - loss: 0.6931 - accuracy: 0.5083\n","Epoch 6/10\n","129/129 [==============================] - 24s 187ms/step - loss: 0.6931 - accuracy: 0.5083\n","Epoch 7/10\n","129/129 [==============================] - 24s 184ms/step - loss: 0.6930 - accuracy: 0.5083\n","Epoch 8/10\n","129/129 [==============================] - 24s 183ms/step - loss: 0.6930 - accuracy: 0.5083\n","Epoch 9/10\n","129/129 [==============================] - 23s 179ms/step - loss: 0.6931 - accuracy: 0.5083\n","Epoch 10/10\n","129/129 [==============================] - 24s 184ms/step - loss: 0.6931 - accuracy: 0.5083\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_3 (Conv2D)           (None, 150, 150, 32)      896       \n","                                                                 \n"," max_pooling2d_3 (MaxPoolin  (None, 75, 75, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 73, 73, 32)        9248      \n","                                                                 \n"," max_pooling2d_4 (MaxPoolin  (None, 36, 36, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 34, 34, 64)        18496     \n","                                                                 \n"," max_pooling2d_5 (MaxPoolin  (None, 17, 17, 64)        0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_1 (Flatten)         (None, 18496)             0         \n","                                                                 \n"," dense_2 (Dense)             (None, 64)                1183808   \n","                                                                 \n"," dropout_1 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 1212513 (4.63 MB)\n","Trainable params: 1212513 (4.63 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# Model params\n","input_shape = (150, 150, 3)\n","num_classes = 1 # Number of outputs per input\n","\n","# Model trained on training set\n","model_our_HVoice = create_cnn_model(input_shape, num_classes)\n","model_our_HVoice.compile(optimizer=RMSprop(learning_rate=0.001),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model_our_HVoice.fit(train_generator_HVoice, epochs=10)\n","model_our_HVoice.summary()  # Print model summary\n","model_our_HVoice.save(root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Our-HVoice')"]},{"cell_type":"markdown","metadata":{"id":"voBmG2uow6kP"},"source":["## Our Model - H-Voice + SiF-DeepVC (Regular)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4360449,"status":"ok","timestamp":1714425888369,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"59ho9cFZxDc8","outputId":"b9be6a92-41b9-4de2-d377-71960a0c326c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","260/260 [==============================] - 1353s 5s/step - loss: 0.5717 - accuracy: 0.6669\n","Epoch 2/10\n","260/260 [==============================] - 325s 1s/step - loss: 0.4062 - accuracy: 0.7746\n","Epoch 3/10\n","260/260 [==============================] - 320s 1s/step - loss: 0.3630 - accuracy: 0.8186\n","Epoch 4/10\n","260/260 [==============================] - 321s 1s/step - loss: 0.2924 - accuracy: 0.8747\n","Epoch 5/10\n","260/260 [==============================] - 318s 1s/step - loss: 0.2639 - accuracy: 0.8927\n","Epoch 6/10\n","260/260 [==============================] - 314s 1s/step - loss: 0.2300 - accuracy: 0.9120\n","Epoch 7/10\n","260/260 [==============================] - 317s 1s/step - loss: 0.2059 - accuracy: 0.9251\n","Epoch 8/10\n","260/260 [==============================] - 315s 1s/step - loss: 0.1945 - accuracy: 0.9290\n","Epoch 9/10\n","260/260 [==============================] - 316s 1s/step - loss: 0.1924 - accuracy: 0.9339\n","Epoch 10/10\n","260/260 [==============================] - 317s 1s/step - loss: 0.1746 - accuracy: 0.9424\n","Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_12 (Conv2D)          (None, 150, 150, 32)      896       \n","                                                                 \n"," max_pooling2d_12 (MaxPooli  (None, 75, 75, 32)        0         \n"," ng2D)                                                           \n","                                                                 \n"," dropout_10 (Dropout)        (None, 75, 75, 32)        0         \n","                                                                 \n"," conv2d_13 (Conv2D)          (None, 73, 73, 32)        9248      \n","                                                                 \n"," max_pooling2d_13 (MaxPooli  (None, 36, 36, 32)        0         \n"," ng2D)                                                           \n","                                                                 \n"," dropout_11 (Dropout)        (None, 36, 36, 32)        0         \n","                                                                 \n"," conv2d_14 (Conv2D)          (None, 34, 34, 64)        18496     \n","                                                                 \n"," max_pooling2d_14 (MaxPooli  (None, 17, 17, 64)        0         \n"," ng2D)                                                           \n","                                                                 \n"," dropout_12 (Dropout)        (None, 17, 17, 64)        0         \n","                                                                 \n"," flatten_4 (Flatten)         (None, 18496)             0         \n","                                                                 \n"," dense_8 (Dense)             (None, 64)                1183808   \n","                                                                 \n"," dropout_13 (Dropout)        (None, 64)                0         \n","                                                                 \n"," dense_9 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 1212513 (4.63 MB)\n","Trainable params: 1212513 (4.63 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# Model params\n","input_shape = (150, 150, 3)\n","num_classes = 1 # Number of outputs per input\n","\n","# Model trained on training set\n","model_our_H_Voice_SiF_Reg = create_new_cnn_model(input_shape, num_classes)\n","model_our_H_Voice_SiF_Reg.compile(optimizer=RMSprop(learning_rate=0.001),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model_our_H_Voice_SiF_Reg.fit(train_generator_H_Reg, epochs=10)\n","model_our_H_Voice_SiF_Reg.summary()  # Print model summary\n","model_our_H_Voice_SiF_Reg.save(root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Our-H-Voice_SiF-Regular')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15969,"status":"ok","timestamp":1714330767002,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"QSlDeGeCoDSH","outputId":"3b1ae80e-8d1f-4caf-9487-c41fdc7dd66c"},"outputs":[{"name":"stdout","output_type":"stream","text":["['hist_0.png', 'hist_1.png', 'hist_2.png', 'hist_3.png', 'hist_6.png', 'hist_7.png', 'hist_8.png', 'hist_17.png', 'hist_18.png', 'hist_22.png', 'hist_23.png', 'hist_25.png', 'hist_27.png', 'hist_28.png', 'hist_31.png', 'hist_32.png', 'hist_34.png', 'hist_35.png', 'hist_37.png', 'hist_38.png', 'hist_42.png', 'hist_43.png', 'hist_45.png', 'hist_49.png', 'hist_51.png', 'hist_53.png', 'hist_54.png', 'hist_56.png', 'hist_57.png', 'hist_59.png', 'hist_60.png', 'hist_61.png', 'hist_62.png', 'hist_63.png', 'hist_64.png', 'hist_66.png', 'hist_68.png', 'hist_69.png', 'hist_72.png', 'hist_73.png', 'hist_74.png', 'hist_76.png', 'hist_77.png', 'hist_78.png', 'hist_83.png', 'hist_84.png', 'hist_85.png', 'hist_86.png', 'hist_88.png', 'hist_89.png', 'hist_91.png', 'hist_92.png', 'hist_95.png', 'hist_96.png', 'hist_98.png', 'hist_99.png', 'hist_100.png', 'hist_107.png', 'hist_110.png', 'hist_114.png', 'hist_116.png', 'hist_118.png', 'hist_122.png', 'hist_124.png', 'hist_126.png', 'hist_127.png', 'hist_129.png', 'hist_130.png', 'hist_132.png', 'hist_135.png', 'hist_136.png', 'hist_137.png', 'hist_141.png', 'hist_142.png', 'hist_143.png', 'hist_146.png', 'hist_147.png', 'hist_151.png', 'hist_154.png', 'hist_156.png', 'hist_161.png', 'hist_163.png', 'hist_164.png', 'hist_165.png', 'hist_168.png', 'hist_170.png', 'hist_174.png', 'hist_175.png', 'hist_183.png', 'hist_186.png', 'hist_187.png', 'hist_190.png', 'hist_194.png', 'hist_195.png', 'hist_199.png', 'hist_201.png', 'hist_202.png', 'hist_204.png', 'hist_206.png', 'hist_207.png', 'hist_208.png', 'hist_209.png', 'hist_210.png', 'hist_212.png', 'hist_213.png', 'hist_214.png', 'hist_215.png', 'hist_218.png', 'hist_219.png', 'hist_220.png', 'hist_222.png', 'hist_224.png', 'hist_225.png', 'hist_226.png', 'hist_227.png', 'hist_228.png', 'hist_232.png', 'hist_234.png', 'hist_235.png', 'hist_237.png', 'hist_238.png', 'hist_246.png', 'hist_248.png', 'hist_249.png', 'hist_252.png', 'hist_254.png', 'hist_256.png', 'hist_257.png', 'hist_258.png', 'hist_259.png', 'hist_261.png', 'hist_262.png', 'hist_263.png', 'hist_264.png', 'hist_265.png', 'hist_266.png', 'hist_267.png', 'hist_268.png', 'hist_269.png', 'hist_272.png', 'hist_273.png', 'hist_274.png', 'hist_275.png', 'hist_276.png', 'hist_277.png', 'hist_279.png', 'hist_283.png', 'hist_285.png', 'hist_287.png', 'hist_288.png', 'hist_289.png', 'hist_291.png', 'hist_294.png', 'hist_295.png', 'hist_297.png', 'hist_300.png', 'hist_301.png', 'hist_302.png', 'hist_309.png', 'hist_312.png', 'hist_313.png', 'hist_314.png', 'hist_316.png', 'hist_318.png', 'hist_319.png', 'hist_320.png', 'hist_323.png', 'hist_324.png', 'hist_326.png', 'hist_328.png', 'hist_329.png', 'hist_332.png', 'hist_337.png', 'hist_341.png', 'hist_342.png', 'hist_348.png', 'hist_351.png', 'hist_352.png', 'hist_353.png', 'hist_355.png', 'hist_356.png', 'hist_357.png', 'hist_358.png', 'hist_359.png', 'hist_361.png', 'hist_362.png', 'hist_364.png', 'hist_367.png', 'hist_370.png', 'hist_372.png', 'hist_373.png', 'hist_375.png', 'hist_380.png', 'hist_381.png', 'hist_382.png', 'hist_387.png', 'hist_389.png', 'hist_392.png', 'hist_393.png', 'hist_394.png', 'hist_396.png', 'hist_400.png', 'hist_407.png', 'hist_408.png', 'hist_409.png', 'hist_410.png', 'hist_413.png', 'hist_417.png', 'hist_418.png', 'hist_419.png', 'hist_422.png', 'hist_423.png', 'hist_424.png', 'hist_425.png', 'hist_427.png', 'hist_429.png', 'hist_432.png', 'hist_435.png', 'hist_436.png', 'hist_440.png', 'hist_441.png', 'hist_443.png', 'hist_445.png', 'hist_446.png', 'hist_447.png', 'hist_449.png', 'hist_451.png', 'hist_453.png', 'hist_454.png', 'hist_457.png', 'hist_460.png', 'hist_464.png', 'hist_465.png', 'hist_467.png', 'hist_468.png', 'hist_469.png', 'hist_474.png', 'hist_476.png', 'hist_477.png', 'hist_478.png', 'hist_481.png', 'hist_483.png', 'hist_486.png', 'hist_489.png', 'hist_491.png', 'hist_495.png', 'hist_499.png', 'hist_500.png', 'hist_501.png', 'hist_503.png', 'hist_506.png', 'hist_507.png', 'hist_509.png', 'hist_510.png', 'hist_511.png', 'hist_514.png', 'hist_516.png', 'hist_517.png', 'hist_520.png', 'hist_522.png', 'hist_523.png', 'hist_524.png', 'hist_525.png', 'hist_526.png', 'hist_530.png', 'hist_533.png', 'hist_537.png', 'hist_539.png', 'hist_540.png', 'hist_542.png', 'hist_543.png', 'hist_544.png', 'hist_546.png', 'hist_548.png', 'hist_549.png', 'hist_551.png', 'hist_555.png', 'hist_564.png', 'hist_565.png', 'hist_567.png', 'hist_568.png', 'hist_571.png', 'hist_572.png', 'hist_573.png', 'hist_574.png', 'hist_575.png', 'hist_577.png', 'hist_580.png', 'hist_581.png', 'hist_582.png', 'hist_585.png', 'hist_587.png', 'hist_588.png', 'hist_591.png', 'hist_592.png', 'hist_595.png', 'hist_598.png', 'hist_601.png', 'hist_602.png', 'hist_604.png', 'hist_606.png', 'hist_609.png', 'hist_610.png', 'hist_611.png', 'hist_612.png', 'hist_614.png', 'hist_615.png', 'hist_617.png', 'hist_618.png', 'hist_624.png', 'hist_625.png', 'hist_626.png', 'hist_628.png', 'hist_630.png', 'hist_632.png', 'hist_634.png', 'hist_635.png', 'hist_639.png', 'hist_644.png', 'hist_649.png', 'hist_650.png', 'hist_651.png', 'hist_653.png', 'hist_656.png', 'hist_657.png', 'hist_660.png', 'hist_662.png', 'hist_664.png', 'hist_666.png', 'hist_667.png', 'hist_672.png', 'hist_673.png', 'hist_674.png', 'hist_675.png', 'hist_676.png', 'hist_679.png', 'hist_682.png', 'hist_684.png', 'hist_686.png', 'hist_688.png', 'hist_690.png', 'hist_692.png', 'hist_693.png', 'hist_700.png', 'hist_701.png', 'hist_704.png', 'hist_705.png', 'hist_706.png', 'hist_708.png', 'hist_709.png', 'hist_713.png', 'hist_717.png', 'hist_719.png', 'hist_723.png', 'hist_725.png', 'hist_728.png', 'hist_732.png', 'hist_733.png', 'hist_734.png', 'hist_736.png', 'hist_737.png', 'hist_738.png', 'hist_740.png', 'hist_741.png', 'hist_742.png', 'hist_743.png', 'hist_744.png', 'hist_746.png', 'hist_747.png', 'hist_748.png', 'hist_749.png', 'hist_750.png', 'hist_751.png', 'hist_753.png', 'hist_754.png', 'hist_755.png', 'hist_757.png', 'hist_758.png', 'hist_759.png', 'hist_760.png', 'hist_761.png', 'hist_763.png', 'hist_764.png', 'hist_767.png', 'hist_769.png', 'hist_773.png', 'hist_777.png', 'hist_778.png', 'hist_779.png', 'hist_783.png', 'hist_785.png', 'hist_787.png', 'hist_788.png', 'hist_790.png', 'hist_791.png', 'hist_793.png', 'hist_798.png', 'hist_800.png', 'hist_801.png', 'hist_802.png', 'hist_803.png', 'hist_805.png', 'hist_806.png', 'hist_807.png', 'hist_811.png', 'hist_812.png', 'hist_813.png', 'hist_814.png', 'hist_818.png', 'hist_820.png', 'hist_822.png', 'hist_827.png', 'hist_831.png', 'hist_833.png', 'hist_836.png', 'hist_838.png', 'hist_839.png', 'hist_840.png', 'hist_842.png', 'hist_843.png', 'hist_846.png', 'hist_847.png', 'hist_848.png', 'hist_851.png', 'hist_852.png', 'hist_853.png', 'hist_858.png', 'hist_859.png', 'hist_860.png', 'hist_861.png', 'hist_863.png', 'hist_864.png', 'hist_867.png', 'hist_868.png', 'hist_870.png', 'hist_872.png', 'hist_876.png', 'hist_878.png', 'hist_879.png', 'hist_880.png', 'hist_883.png', 'hist_888.png', 'hist_889.png', 'hist_891.png', 'hist_894.png', 'hist_895.png', 'hist_896.png', 'hist_899.png', 'hist_901.png', 'hist_903.png', 'hist_906.png', 'hist_909.png', 'hist_912.png', 'hist_913.png', 'hist_916.png', 'hist_918.png', 'hist_919.png', 'hist_920.png', 'hist_921.png', 'hist_925.png', 'hist_931.png', 'hist_932.png', 'hist_933.png', 'hist_934.png', 'hist_937.png', 'hist_938.png', 'hist_940.png', 'hist_945.png', 'hist_946.png', 'hist_947.png', 'hist_949.png', 'hist_950.png', 'hist_953.png', 'hist_954.png', 'hist_955.png', 'hist_956.png', 'hist_959.png', 'hist_961.png', 'hist_962.png', 'hist_964.png', 'hist_965.png', 'hist_966.png', 'hist_972.png', 'hist_975.png', 'hist_977.png', 'hist_978.png', 'hist_979.png', 'hist_982.png', 'hist_983.png', 'hist_984.png', 'hist_985.png', 'hist_986.png', 'hist_991.png', 'hist_995.png', 'hist_996.png', 'hist_997.png', 'hist_998.png', 'hist_999.png', 'hist_1004.png', 'hist_1006.png', 'hist_1007.png', 'hist_1009.png', 'hist_1010.png', 'hist_1011.png', 'hist_1012.png', 'hist_1014.png', 'hist_1018.png', 'hist_1020.png', 'hist_1025.png', 'hist_1027.png', 'hist_1032.png', 'hist_1033.png', 'hist_1034.png', 'hist_1035.png', 'hist_1036.png', 'hist_1040.png', 'hist_1041.png', 'hist_1043.png', 'hist_1044.png', 'hist_1045.png', 'hist_1046.png', 'hist_1047.png', 'hist_1050.png', 'hist_1052.png', 'hist_1056.png', 'hist_1058.png', 'hist_1060.png', 'hist_1062.png', 'hist_1063.png', 'hist_1065.png', 'hist_1066.png', 'hist_1070.png', 'hist_1071.png', 'hist_1073.png', 'hist_1074.png', 'hist_1077.png', 'hist_1079.png', 'hist_1081.png', 'hist_1082.png', 'hist_1084.png', 'hist_1085.png', 'hist_1090.png', 'hist_1093.png', 'hist_1095.png', 'hist_1096.png', 'hist_1099.png', 'hist_1103.png', 'hist_1104.png', 'hist_1105.png', 'hist_1106.png', 'hist_1108.png', 'hist_1111.png', 'hist_1112.png', 'hist_1117.png', 'hist_1118.png', 'hist_1121.png', 'hist_1123.png', 'hist_1124.png', 'hist_1128.png', 'hist_1131.png', 'hist_1133.png', 'hist_1135.png', 'hist_1138.png', 'hist_1141.png', 'hist_1142.png', 'hist_1144.png', 'hist_1145.png', 'hist_1152.png', 'hist_1156.png', 'hist_1157.png', 'hist_1158.png', 'hist_1160.png', 'hist_1161.png', 'hist_1163.png', 'hist_1165.png', 'hist_1166.png', 'hist_1167.png', 'hist_1168.png', 'hist_1169.png', 'hist_1173.png', 'hist_1176.png', 'hist_1180.png', 'hist_1183.png', 'hist_1184.png', 'hist_1186.png', 'hist_1187.png', 'hist_1189.png', 'hist_1190.png', 'hist_1192.png', 'hist_1194.png', 'hist_1195.png', 'hist_1198.png', 'hist_1199.png', 'hist_1200.png', 'hist_1206.png', 'hist_1208.png', 'hist_1209.png', 'hist_1210.png', 'hist_1211.png', 'hist_1213.png', 'hist_1215.png', 'hist_1217.png', 'hist_1219.png', 'hist_1221.png', 'hist_1224.png', 'hist_1225.png', 'hist_1230.png', 'hist_1231.png', 'hist_1233.png', 'hist_1234.png', 'hist_1235.png', 'hist_1236.png', 'hist_1238.png', 'hist_1241.png', 'hist_1242.png', 'hist_1243.png', 'hist_1248.png', 'hist_1250.png', 'hist_1252.png', 'hist_1253.png', 'hist_1259.png', 'hist_1261.png', 'hist_1262.png', 'hist_1263.png', 'hist_1265.png', 'hist_1267.png', 'hist_1268.png', 'hist_1269.png', 'hist_1271.png', 'hist_1273.png', 'hist_1274.png', 'hist_1276.png', 'hist_1277.png', 'hist_1278.png', 'hist_1279.png', 'hist_1280.png', 'hist_1281.png', 'hist_1282.png', 'hist_1284.png', 'hist_1286.png', 'hist_1287.png', 'hist_1288.png', 'hist_1290.png', 'hist_1293.png', 'hist_1297.png', 'hist_1298.png', 'hist_1300.png', 'hist_1303.png', 'hist_1304.png', 'hist_1305.png', 'hist_1308.png', 'hist_1312.png', 'hist_1313.png', 'hist_1314.png', 'hist_1316.png', 'hist_1317.png', 'hist_1318.png', 'hist_1319.png', 'hist_1320.png', 'hist_1322.png', 'hist_1323.png', 'hist_1324.png', 'hist_1327.png', 'hist_1328.png', 'hist_1330.png', 'hist_1331.png', 'hist_1333.png', 'hist_1334.png', 'hist_1335.png', 'hist_1338.png', 'hist_1340.png', 'hist_1348.png', 'hist_1349.png']\n"]}],"source":["import os\n","from PIL import Image\n","directory = root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Test_Set/real'\n","files = os.listdir(directory)\n","print(files)\n","\n","\n","for file in files:\n","    try:\n","        img = Image.open(os.path.join(directory, file))\n","    except Exception as e:\n","        print(f\"Error opening {file}: {e}\")"]},{"cell_type":"markdown","metadata":{"id":"93SySYw8xFXv"},"source":["## Our Model - H-Voice + SiF-DeepVC (Filtered)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4692734,"status":"ok","timestamp":1714340086189,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"a3T3jtb_xLoM","outputId":"3bfc7d18-21b4-43ed-b4d6-999863b0d154"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","260/260 [==============================] - 2188s 8s/step - loss: 0.4745 - accuracy: 0.7324\n","Epoch 2/10\n","260/260 [==============================] - 268s 1s/step - loss: 0.3538 - accuracy: 0.8212\n","Epoch 3/10\n","260/260 [==============================] - 267s 1s/step - loss: 0.2537 - accuracy: 0.8916\n","Epoch 4/10\n","260/260 [==============================] - 268s 1s/step - loss: 0.1769 - accuracy: 0.9353\n","Epoch 5/10\n","260/260 [==============================] - 264s 1s/step - loss: 0.1434 - accuracy: 0.9505\n","Epoch 6/10\n","260/260 [==============================] - 266s 1s/step - loss: 0.1188 - accuracy: 0.9605\n","Epoch 7/10\n","260/260 [==============================] - 264s 1s/step - loss: 0.1122 - accuracy: 0.9663\n","Epoch 8/10\n","260/260 [==============================] - 264s 1s/step - loss: 0.0984 - accuracy: 0.9665\n","Epoch 9/10\n","260/260 [==============================] - 263s 1s/step - loss: 0.0962 - accuracy: 0.9706\n","Epoch 10/10\n","260/260 [==============================] - 262s 1s/step - loss: 0.0881 - accuracy: 0.9719\n","Model: \"sequential_9\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_27 (Conv2D)          (None, 150, 150, 32)      896       \n","                                                                 \n"," max_pooling2d_27 (MaxPooli  (None, 75, 75, 32)        0         \n"," ng2D)                                                           \n","                                                                 \n"," conv2d_28 (Conv2D)          (None, 73, 73, 32)        9248      \n","                                                                 \n"," max_pooling2d_28 (MaxPooli  (None, 36, 36, 32)        0         \n"," ng2D)                                                           \n","                                                                 \n"," conv2d_29 (Conv2D)          (None, 34, 34, 64)        18496     \n","                                                                 \n"," max_pooling2d_29 (MaxPooli  (None, 17, 17, 64)        0         \n"," ng2D)                                                           \n","                                                                 \n"," flatten_9 (Flatten)         (None, 18496)             0         \n","                                                                 \n"," dense_18 (Dense)            (None, 64)                1183808   \n","                                                                 \n"," dropout_33 (Dropout)        (None, 64)                0         \n","                                                                 \n"," dense_19 (Dense)            (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 1212513 (4.63 MB)\n","Trainable params: 1212513 (4.63 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# Model params\n","input_shape = (150, 150, 3)\n","num_classes = 1 # Number of outputs per input\n","\n","# Model trained on training set\n","model_our_SiF_filt = create_cnn_model(input_shape, num_classes)\n","model_our_SiF_filt.compile(optimizer=RMSprop(learning_rate=0.001),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model_our_SiF_filt.fit(train_generator_H_Filt, epochs=10)\n","model_our_SiF_filt.summary()  # Print model summary\n","model_our_SiF_filt.save(root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Our-H-Voice_SiF-Filtered')"]},{"cell_type":"markdown","metadata":{"id":"SkfviSLUtVG9"},"source":["## Model Stacking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0vV7xGcqtYJh"},"outputs":[],"source":["# Implement stacking\n"]},{"cell_type":"markdown","metadata":{"id":"QAkKHdrcUTKb"},"source":["# Performances"]},{"cell_type":"markdown","metadata":{"id":"diZ5MJ91IXaL"},"source":["## on SiF-DeepVC Data - Regular"]},{"cell_type":"markdown","metadata":{"id":"uK7d1mxgwKZd"},"source":["### Original-HVoice"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17449,"status":"ok","timestamp":1710921942581,"user":{"displayName":"Shuang Lin","userId":"13512369623226330440"},"user_tz":240},"id":"9A02p3L84r1u","outputId":"dc9340d8-8940-47b9-d5cb-b85fee08531e"},"outputs":[{"name":"stdout","output_type":"stream","text":["43/43 [==============================] - 17s 381ms/step - loss: 13.9775 - tp: 691.0000 - fp: 641.0000 - tn: 16.0000 - fn: 2.0000 - accuracy: 0.5237 - precision: 0.5188 - recall: 0.9971 - auc: 0.5345\n","Validation accuracy: 52.37036943435669%\n"]}],"source":["# Validation Set\n","evaluation_results = model_original_HVoice.evaluate(valid_generator_SiF_reg)\n","print(f'Validation accuracy: {evaluation_results[5] * 100}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17792,"status":"ok","timestamp":1710921960350,"user":{"displayName":"Shuang Lin","userId":"13512369623226330440"},"user_tz":240},"id":"M-QgZuVF4uXX","outputId":"7d22f765-d022-457c-d760-a667a1502175"},"outputs":[{"name":"stdout","output_type":"stream","text":["43/43 [==============================] - 17s 396ms/step - loss: 15.8890 - tp: 643.0000 - fp: 685.0000 - tn: 21.0000 - fn: 1.0000 - accuracy: 0.4919 - precision: 0.4842 - recall: 0.9984 - auc: 0.5258\n","Test accuracy: 49.18518662452698%\n"]}],"source":["# Test set\n","evaluation_results = model_original.evaluate(test_generator_SiF_reg)\n","print(f'Test accuracy: {evaluation_results[5] * 100}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12926,"status":"ok","timestamp":1710921568572,"user":{"displayName":"Shuang Lin","userId":"13512369623226330440"},"user_tz":240},"id":"zPfKefSzwX7c","outputId":"ededc1f3-995e-4767-de8f-5bf1c5662409"},"outputs":[{"name":"stdout","output_type":"stream","text":["32/32 [==============================] - 12s 380ms/step - loss: 28.8742 - tp: 0.0000e+00 - fp: 986.0000 - tn: 14.0000 - fn: 0.0000e+00 - accuracy: 0.0140 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.0000e+00\n","Original Deep4SNet Test accuracy: 1.4000000432133675%\n"]}],"source":["# Test on fake voices built specifically against Deep4SNet by SiF-DeepVC\n","evaluation_results = model_original.evaluate(deep4s_generator_SiF_reg)\n","print(f'Original Deep4SNet Test accuracy: {evaluation_results[5] * 100}%')"]},{"cell_type":"markdown","metadata":{"id":"RCb7Y8PwwG-q"},"source":["### Our-SiF-Regular"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1055537,"status":"ok","timestamp":1713040505778,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"ay7IPOfvIaWn","outputId":"bed1b361-60a2-4d3f-cc39-8f113445a8fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["83/83 [==============================] - 1034s 13s/step - loss: 0.1966 - accuracy: 0.9346\n","Validation accuracy: 93.45510005950928%\n"]}],"source":["# Validation Set\n","loss_val, accuracy_val = model_our.evaluate(valid_generator_SiF_reg)\n","print(f'Validation accuracy: {accuracy_val * 100}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395187,"status":"ok","timestamp":1713040978154,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"Yi1ZZ6RpK8iP","outputId":"b8d32515-9c7b-4686-ef74-aef3cb6ea368"},"outputs":[{"name":"stdout","output_type":"stream","text":["29/29 [==============================] - 349s 12s/step - loss: 0.2351 - accuracy: 0.9222\n","Test accuracy: 92.22221970558167%\n"]}],"source":["# Test set\n","loss_test, accuracy_test = model_our.evaluate(test_generator_SiF_reg)\n","print(f'Test accuracy: {accuracy_test * 100}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"executionInfo":{"elapsed":173,"status":"error","timestamp":1713040579950,"user":{"displayName":"Thomas Snipes","userId":"15414075403793182525"},"user_tz":240},"id":"-d17ZdNVSlBm","outputId":"d12246ba-39cd-4b39-d86f-217c38359a46"},"outputs":[],"source":["# Test on fake voices built specifically against Deep4SNet by SiF-DeepVC\n","loss_test, accuracy_test = model_our.evaluate(deep4s_generator_SiF_reg)\n","print(f'Our Deep4SNet Test accuracy: {accuracy_test * 100}%')"]},{"cell_type":"markdown","metadata":{"id":"VFhskoX4UjQW"},"source":["## on SiF-DeepVC - Filtered (WIP)"]},{"cell_type":"markdown","metadata":{"id":"j7HBl20xTRwg"},"source":["## on H-Voice"]},{"cell_type":"markdown","metadata":{"id":"KzpITJxcSIQg"},"source":["### Our-SiF-Regular"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"er00V4Ch5_eI"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","\n","# ---------- Load our saved model ---------- #\n","model_path = root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Our-SiF-Regular'\n","#model_path = root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Our' # <- Old one. Renamed with SiF-Regular for clarity\n","loaded_model_our_SiF_regular = load_model(model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":390491,"status":"ok","timestamp":1713244153454,"user":{"displayName":"Shuang Lin","userId":"13512369623226330440"},"user_tz":240},"id":"jKtPOCFvTXHm","outputId":"ce24c83b-d509-4381-d65b-a6f532c05a03"},"outputs":[{"name":"stdout","output_type":"stream","text":["54/54 [==============================] - 380s 7s/step - loss: 2.5095 - accuracy: 0.4907\n","Validation accuracy: 49.07407462596893%\n"]}],"source":["# Validation Set\n","loss_val, accuracy_val = loaded_model_our_SiF_regular.evaluate(valid_generator_HVoice)\n","print(f'Validation accuracy: {accuracy_val * 100}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HnoLpbnn56au"},"outputs":[],"source":["# Test set\n","loss_test, accuracy_test = loaded_model.evaluate(test_generator_HVoice)\n","print(f'Test accuracy: {accuracy_test * 100}%')"]},{"cell_type":"markdown","metadata":{"id":"HlDF_gMCSUv5"},"source":["### Our-HVoice"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1K34RGMSbzY"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","\n","# ---------- Load our saved model ---------- #\n","model_path = root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Our-HVoice'\n","loaded_model_our_HVoice = load_model(model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9918,"status":"ok","timestamp":1713244164147,"user":{"displayName":"Shuang Lin","userId":"13512369623226330440"},"user_tz":240},"id":"upXkUfDGSn-Q","outputId":"5c519658-4228-48a7-ead2-246b0018b687"},"outputs":[{"name":"stdout","output_type":"stream","text":["54/54 [==============================] - 10s 174ms/step - loss: 0.6933 - accuracy: 0.5000\n","Validation accuracy: 50.0%\n"]}],"source":["# Validation Set\n","loss_val, accuracy_val = loaded_model_our_HVoice.evaluate(valid_generator_HVoice)\n","print(f'Validation accuracy: {accuracy_val * 100}%')"]},{"cell_type":"markdown","metadata":{"id":"qsg-tPW2V4KC"},"source":["### Original-HVoice"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3XpsMboZkB-"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","model_path = root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Original-HVoice/model_Deep4SNet.h5'\n","weights_path = root_dir + 'Voice_Cloning_Detection/Models/Deep4SNet-Original-HVoice/weights_Deep4SNet.h5'\n","model_original_HVoice = load_model(model_path)\n","model_original_HVoice.load_weights(weights_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"executionInfo":{"elapsed":215,"status":"error","timestamp":1713245701569,"user":{"displayName":"Shuang Lin","userId":"13512369623226330440"},"user_tz":240},"id":"ggJII6qvZdw5","outputId":"8db143c4-1c81-42ff-8b55-86b06869be22"},"outputs":[],"source":["# Validation Set\n","loss_val, accuracy_val = model_original_HVoice.evaluate(valid_generator_HVoice)\n","print(f'Validation accuracy: {accuracy_val * 100}%')"]},{"cell_type":"markdown","metadata":{"id":"GQ6T1w8jslGp"},"source":["# Performance Metrics (WIP)\n","- FPR (false positive rate) as used in SiF-DeepVC paper https://ieeexplore.ieee.org/document/10301243"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224374,"status":"ok","timestamp":1710869204857,"user":{"displayName":"Shuang Lin","userId":"13512369623226330440"},"user_tz":240},"id":"rl2zIy4zsxls","outputId":"09ed7c48-b40a-426c-d0af-0435a8edecdf"},"outputs":[{"name":"stdout","output_type":"stream","text":["29/29 [==============================] - 214s 8s/step\n","False Positive Rate (FPR): 0.21810089020771514\n"]}],"source":["from sklearn.metrics import confusion_matrix\n","\n","# Predict probabilities for the test set\n","y_pred_proba = model.predict(test_generator) # ----- CHANGE for Models ---------------\n","\n","# Convert probabilities to binary predictions (0 or 1) based on a threshold\n","threshold = 0.5  # You can adjust this threshold if needed\n","y_pred_binary = (y_pred_proba > threshold).astype(int)\n","\n","# Get true labels for the test set\n","y_true = test_generator.classes # ----- CHANGE for Models ---------------\n","\n","# Compute confusion matrix\n","conf_matrix = confusion_matrix(y_true, y_pred_binary)\n","\n","# Extract true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP) from the confusion matrix\n","TN = conf_matrix[0, 0]\n","FP = conf_matrix[0, 1]\n","FN = conf_matrix[1, 0]\n","TP = conf_matrix[1, 1]\n","\n","# Compute false positive rate (FPR)\n","FPR = FP / (FP + TN)\n","\n","print(\"False Positive Rate (FPR):\", FPR)\n"]}],"metadata":{"colab":{"collapsed_sections":["qBqwpqQI9H-s","KbpeiTpH6OV3","BCPg229mGNRT","pW8BFSlRD0v3","GuMznXbEGHZ9","cShfFKg_qheO","jgSK2LUssJ7q","xJpYr9ECXUJI","gfpp0gybcWBo","YaLtJPW6rydg","hQSG2b3LJhis","gtJn2rwv39kh","1bmQ901jpe1r","voBmG2uow6kP","uK7d1mxgwKZd","RCb7Y8PwwG-q"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
